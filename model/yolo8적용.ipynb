{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0abb2a1f-6e43-461f-affc-8a0bff69cd20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralytics\n",
      "  Using cached ultralytics-8.3.152-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from ultralytics) (3.9.2)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from ultralytics) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from ultralytics) (10.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from ultralytics) (1.13.1)\n",
      "Collecting torch>=1.8.0 (from ultralytics)\n",
      "  Using cached torch-2.7.1-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting torchvision>=0.9.0 (from ultralytics)\n",
      "  Using cached torchvision-0.22.1-cp312-cp312-win_amd64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from ultralytics) (4.66.5)\n",
      "Requirement already satisfied: psutil in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from ultralytics) (5.9.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from ultralytics) (2.2.2)\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
      "  Using cached ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n",
      "Requirement already satisfied: filelock in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.14.0)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
      "Using cached ultralytics-8.3.152-py3-none-any.whl (1.0 MB)\n",
      "Using cached torch-2.7.1-cp312-cp312-win_amd64.whl (216.1 MB)\n",
      "Using cached torchvision-0.22.1-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "Using cached ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Installing collected packages: sympy, torch, ultralytics-thop, torchvision, ultralytics\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.2\n",
      "    Uninstalling sympy-1.13.2:\n",
      "      Successfully uninstalled sympy-1.13.2\n",
      "Successfully installed sympy-1.14.0 torch-2.7.1 torchvision-0.22.1 ultralytics-8.3.152 ultralytics-thop-2.0.14\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23891a1b-e2b1-4e2f-bc74-ee4d4e05dea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 이미지 수: 3000\n",
      "  → Train: 2400장 (비율 80%)\n",
      "  → Val:   600장 (비율 20%)\n"
     ]
    }
   ],
   "source": [
    "#train, val 8:2\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def split_dataset(root_dir, train_ratio=0.8, seed=42):\n",
    "    # 1) 원본 이미지/레이블 디렉터리 경로\n",
    "    images_dir = os.path.join(\"C:/Users/jenni/Desktop/final_data1/train\", \"images\")\n",
    "    labels_dir = os.path.join(\"C:/Users/jenni/Desktop/final_data1/train\", \"labels\")\n",
    "\n",
    "    # 2) train/val 이미지 & 레이블을 저장할 디렉터리 경로\n",
    "    train_images_dir = os.path.join(\"C:/Users/jenni/Desktop/final_data1\", \"train1\", \"images\")\n",
    "    train_labels_dir = os.path.join(\"C:/Users/jenni/Desktop/final_data1\", \"train1\", \"labels\")\n",
    "    val_images_dir = os.path.join(\"C:/Users/jenni/Desktop/final_data1\", \"val\", \"images\")\n",
    "    val_labels_dir = os.path.join(\"C:/Users/jenni/Desktop/final_data1\", \"val\", \"labels\")\n",
    "\n",
    "    train_ratio = 0.8 \n",
    "    # 3) train/val 디렉터리 생성 (이미 있으면 무시)\n",
    "    for d in (train_images_dir, train_labels_dir, val_images_dir, val_labels_dir):\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    # 4) images_dir 안에 있는 모든 파일명을 리스트로 가져오기\n",
    "    #    (폴더 안에 이미지가 .jpg, .png, 기타 다른 확장자로 섞여 있어도 모두 포함)\n",
    "    all_images = [\n",
    "        f\n",
    "        for f in os.listdir(images_dir)\n",
    "        if os.path.isfile(os.path.join(images_dir, f))\n",
    "    ]\n",
    "\n",
    "    if len(all_images) == 0:\n",
    "        raise RuntimeError(f\"이미지 폴더에 파일이 하나도 없습니다: {images_dir}\")\n",
    "\n",
    "    # 5) 랜덤으로 셔플하고 split index 계산\n",
    "    random.seed(seed)\n",
    "    random.shuffle(all_images)\n",
    "    split_idx = int(len(all_images) * train_ratio)\n",
    "\n",
    "    train_images = all_images[:split_idx]\n",
    "    val_images = all_images[split_idx:]\n",
    "\n",
    "    # 6) train 이미지/레이블 복사\n",
    "    for img_file in train_images:\n",
    "        # -- 이미지 파일 복사\n",
    "        src_img = os.path.join(images_dir, img_file)\n",
    "        dst_img = os.path.join(train_images_dir, img_file)\n",
    "        shutil.copy2(src_img, dst_img)\n",
    "\n",
    "        # -- 레이블 파일(.txt) 복사\n",
    "        base_name, _ = os.path.splitext(img_file)\n",
    "        label_file = base_name + \".txt\"\n",
    "        src_lbl = os.path.join(labels_dir, label_file)\n",
    "        dst_lbl = os.path.join(train_labels_dir, label_file)\n",
    "        if os.path.exists(src_lbl):\n",
    "            shutil.copy2(src_lbl, dst_lbl)\n",
    "        else:\n",
    "            print(f\"Warning: train 레이블 파일을 찾을 수 없습니다: {src_lbl}\")\n",
    "\n",
    "    # 7) val 이미지/레이블 복사\n",
    "    for img_file in val_images:\n",
    "        src_img = os.path.join(images_dir, img_file)\n",
    "        dst_img = os.path.join(val_images_dir, img_file)\n",
    "        shutil.copy2(src_img, dst_img)\n",
    "\n",
    "        base_name, _ = os.path.splitext(img_file)\n",
    "        label_file = base_name + \".txt\"\n",
    "        src_lbl = os.path.join(labels_dir, label_file)\n",
    "        dst_lbl = os.path.join(val_labels_dir, label_file)\n",
    "        if os.path.exists(src_lbl):\n",
    "            shutil.copy2(src_lbl, dst_lbl)\n",
    "        else:\n",
    "            print(f\"Warning: val 레이블 파일을 찾을 수 없습니다: {src_lbl}\")\n",
    "\n",
    "    print(f\"총 이미지 수: {len(all_images)}\")\n",
    "    print(f\"  → Train: {len(train_images)}장 (비율 {train_ratio*100:.0f}%)\")\n",
    "    print(f\"  → Val:   {len(val_images)}장 (비율 {(1-train_ratio)*100:.0f}%)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 실제 root_dir 경로를 본인 PC에 맞게 수정하세요.\n",
    "    # (예: \"C:/Users/USER/Desktop/augmented_data1\")\n",
    "    root_dir = \"C:/Users/jenni/Desktop/final_data1\"\n",
    "    split_dataset(root_dir, train_ratio=0.8, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d35c6d7-330c-46d9-8684-52be4e2112af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "def train_yolo_for_method(method, base_path=\"C:／Users／USER／Desktop／augmented1\", epochs=50):\n",
    "    # 1. 각 증강 방식에 맞는 데이터셋 경로 설정\n",
    "    dataset_path = os.path.join(base_path, method)\n",
    "    yaml_path = os.path.join(dataset_path, \"data.yaml\")\n",
    "    create_data_yaml(dataset_path, yaml_path)\n",
    "\n",
    "    # 2. 항상 동일한 사전 학습 모델로부터 학습 시작\n",
    "    model = YOLO(\"yolov8n-seg.pt\")\n",
    "\n",
    "    # 3. 모든 학습에 동일한 하이퍼파라미터 적용\n",
    "    model.train(\n",
    "        data=yaml_path,\n",
    "        epochs=epochs,       # 에포크 횟수 (50으로 고정)\n",
    "        imgsz=640,           # 이미지 크기 (640으로 고정)\n",
    "        name=f\"yolov8n_{method}\", # 결과 저장 폴더 이름 지정\n",
    "        project=\"C:／Users／USER／Desktop\", # 결과 저장 최상위 경로\n",
    "        save=True,           # 모델 체크포인트 저장\n",
    "        resume=False         # 이어서 학습하지 않고 항상 처음부터 시작\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9f329f-3883-4d93-9081-3e0e0a2b42e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting YOLOv8 training and validation for method: train ---\n",
      "Loaded classes: {0: 'ac', 1: 'lc', 2: 'ph', 3: 'pc', 4: 'tc'}\n",
      "Detected image dimensions: W=1920, H=648\n",
      "Converting Labelme JSON to YOLO segmentation .txt for train...\n",
      "Conversion complete.\n",
      "'C:/Users/jenni/Desktop/augmented_data3\\train\\data.yaml' file created successfully.\n",
      "Loaded YOLOv8n-seg model.\n",
      "Ultralytics 8.3.152  Python-3.11.11 torch-2.7.1+cpu CPU (11th Gen Intel Core(TM) i7-11800H 2.30GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=C:/Users/jenni/Desktop/augmented_data3\\train\\data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n-seg.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8n_train2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=C:/Users/jenni/Desktop/YOLO_TRAIN_RESULTS3, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=C:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS3\\yolov8n_train2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=5\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1005055  ultralytics.nn.modules.head.Segment          [5, 32, 64, [64, 128, 256]]   \n",
      "YOLOv8n-seg summary: 151 layers, 3,264,591 parameters, 3,264,575 gradients, 12.1 GFLOPs\n",
      "\n",
      "Transferred 381/417 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.20.1 ms, read: 31.36.7 MB/s, size: 346.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\jenni\\Desktop\\augmented_data3\\train\\labels.cache... 1000 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1000/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 23.56.5 MB/s, size: 321.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\jenni\\Desktop\\augmented_data3\\train\\labels.cache... 1000 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1000/1000 [00:00<?, ?it/s]\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to C:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS3\\yolov8n_train2\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mC:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS3\\yolov8n_train2\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/30         0G        2.2      4.478      3.211      1.922        131        640: 100%|██████████| 63/63 [06:26<00:00,  6.13s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:06<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.764      0.146      0.137      0.068      0.666     0.0463     0.0298    0.00848\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/30         0G      1.956      3.756      2.391      1.728        114        640: 100%|██████████| 63/63 [06:21<00:00,  6.06s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:09<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.351      0.245      0.201      0.105      0.239      0.133     0.0824     0.0256\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/30         0G      1.911      3.689      2.273      1.695        106        640: 100%|██████████| 63/63 [06:34<00:00,  6.27s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:07<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.444      0.216      0.192     0.0976      0.322      0.176      0.117     0.0385\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/30         0G      1.871      3.629       2.19      1.672        108        640: 100%|██████████| 63/63 [06:50<00:00,  6.51s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:26<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.635      0.263      0.266      0.143      0.587      0.203      0.181     0.0624\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/30         0G      1.824      3.529      2.077      1.636        135        640: 100%|██████████| 63/63 [09:27<00:00,  9.01s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [02:44<00:00,  5.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.419      0.309      0.274      0.148      0.387      0.225      0.184     0.0615\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/30         0G       1.78      3.503      2.052      1.605        126        640: 100%|██████████| 63/63 [13:41<00:00, 13.04s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:14<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.448      0.301      0.299       0.17      0.411      0.244      0.219     0.0772\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/30         0G      1.743      3.458      1.965      1.581         96        640: 100%|██████████| 63/63 [06:20<00:00,  6.04s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:10<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.578        0.3      0.313       0.18      0.532       0.26      0.236     0.0825\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/30         0G      1.727      3.408      1.915      1.577        123        640: 100%|██████████| 63/63 [06:45<00:00,  6.43s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:10<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.516      0.357      0.353       0.21      0.496      0.314      0.287      0.111\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/30         0G      1.697      3.351      1.876      1.544        113        640: 100%|██████████| 63/63 [06:49<00:00,  6.50s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:15<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.597      0.339      0.368      0.222      0.554      0.295      0.287      0.109\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/30         0G      1.671      3.355      1.846      1.534        107        640: 100%|██████████| 63/63 [06:41<00:00,  6.37s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:11<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.494      0.385      0.373      0.225      0.506      0.332      0.303      0.119\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/30         0G      1.663      3.283      1.829      1.531        119        640: 100%|██████████| 63/63 [06:41<00:00,  6.38s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:11<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.619      0.349      0.368      0.223      0.579      0.315      0.312       0.12\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/30         0G       1.63      3.231      1.772      1.518         76        640: 100%|██████████| 63/63 [06:44<00:00,  6.41s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:14<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.531      0.384      0.384      0.232      0.505      0.328      0.299      0.117\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/30         0G      1.617      3.221       1.75      1.504         97        640: 100%|██████████| 63/63 [17:30<00:00, 16.68s/it]   \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:12<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.529      0.397      0.402      0.248      0.525      0.354      0.339      0.134\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/30         0G      1.594      3.215      1.733      1.498         96        640: 100%|██████████| 63/63 [07:05<00:00,  6.76s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:12<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.559      0.388      0.408      0.256      0.497      0.357      0.333      0.129\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/30         0G       1.57      3.169      1.662      1.468        110        640: 100%|██████████| 63/63 [06:37<00:00,  6.31s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:09<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.652      0.395      0.423       0.27      0.589      0.361       0.36      0.144\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/30         0G      1.556      3.148      1.649      1.464        113        640: 100%|██████████| 63/63 [07:12<00:00,  6.86s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:26<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.573      0.426      0.432      0.277      0.552      0.379      0.364      0.146\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/30         0G      1.543      3.133      1.637      1.451         86        640: 100%|██████████| 63/63 [07:20<00:00,  6.99s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:10<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.563      0.427       0.44      0.286      0.535       0.39      0.377      0.157\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/30         0G      1.506      3.125        1.6       1.44         97        640: 100%|██████████| 63/63 [06:15<00:00,  5.96s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:13<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.572      0.429      0.446      0.292      0.544      0.394      0.381      0.152\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/30         0G      1.504      3.106      1.586      1.422         85        640: 100%|██████████| 63/63 [07:01<00:00,  6.69s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:09<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.589      0.426       0.45      0.297      0.555      0.389      0.385      0.162\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/30         0G      1.497      3.084      1.577      1.426        125        640: 100%|██████████| 63/63 [06:35<00:00,  6.27s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:16<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.609      0.437      0.461       0.31      0.596      0.392      0.398      0.171\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "      21/30         0G      1.542      2.987      1.731        1.5         49        640: 100%|██████████| 63/63 [05:56<00:00,  5.67s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:11<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.669      0.402      0.448      0.301      0.655      0.369      0.384      0.162\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/30         0G       1.47      2.906      1.573      1.457         53        640: 100%|██████████| 63/63 [05:57<00:00,  5.67s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:06<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.636      0.436      0.466      0.314      0.629      0.409      0.408      0.179\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/30         0G      1.446      2.878      1.535      1.447         53        640: 100%|██████████| 63/63 [06:01<00:00,  5.73s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:07<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.714      0.432      0.475      0.322      0.697      0.397      0.422      0.184\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/30         0G      1.432      2.858        1.5      1.436         50        640: 100%|██████████| 63/63 [06:06<00:00,  5.81s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:06<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.649       0.45      0.482      0.327      0.624      0.426      0.428      0.188\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/30         0G      1.407      2.834      1.468       1.42         39        640: 100%|██████████| 63/63 [05:56<00:00,  5.65s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:06<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.645       0.46      0.493      0.338      0.659      0.428      0.439      0.194\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/30         0G      1.384      2.804      1.432      1.411         42        640: 100%|██████████| 63/63 [06:00<00:00,  5.72s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:06<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.654      0.461      0.491      0.339      0.644       0.43      0.435      0.189\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/30         0G      1.374      2.776      1.427      1.408         41        640: 100%|██████████| 63/63 [06:03<00:00,  5.77s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:07<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019       0.64      0.478      0.503      0.351      0.666      0.442      0.451        0.2\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/30         0G      1.368      2.778      1.407      1.401         52        640: 100%|██████████| 63/63 [06:01<00:00,  5.74s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:06<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.652      0.475      0.506      0.355      0.637      0.443       0.45      0.203\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/30         0G      1.334      2.773      1.397       1.39         52        640: 100%|██████████| 63/63 [06:00<00:00,  5.73s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:07<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019       0.68      0.475       0.51      0.361      0.678      0.444      0.453      0.203\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/30         0G      1.324      2.739       1.38       1.38         43        640: 100%|██████████| 63/63 [05:43<00:00,  5.46s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:59<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.695      0.475      0.516      0.367      0.693      0.447      0.461      0.209\n",
      "\n",
      "30 epochs completed in 4.202 hours.\n",
      "Optimizer stripped from C:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS3\\yolov8n_train2\\weights\\last.pt, 6.7MB\n",
      "Optimizer stripped from C:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS3\\yolov8n_train2\\weights\\best.pt, 6.7MB\n",
      "\n",
      "Validating C:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS3\\yolov8n_train2\\weights\\best.pt...\n",
      "Ultralytics 8.3.152  Python-3.11.11 torch-2.7.1+cpu CPU (11th Gen Intel Core(TM) i7-11800H 2.30GHz)\n",
      "YOLOv8n-seg summary (fused): 85 layers, 3,259,039 parameters, 0 gradients, 12.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   3%|▎         | 1/32 [00:01<01:01,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   6%|▋         | 2/32 [00:04<01:01,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:52<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       6019      0.694      0.477      0.515      0.367      0.694      0.447      0.461      0.209\n",
      "                    ac        758       1590      0.816      0.889      0.913      0.717      0.798      0.847       0.84      0.421\n",
      "                    lc        612       1165      0.737      0.788      0.806       0.62      0.692      0.712      0.685      0.262\n",
      "                    ph        739       1299      0.671      0.329      0.371      0.207      0.669      0.309      0.342      0.152\n",
      "                    pc        511        772      0.696      0.267      0.296      0.208      0.749      0.275      0.301      0.168\n",
      "                    tc        714       1193      0.548       0.11      0.191     0.0822      0.562      0.093      0.138     0.0425\n",
      "Speed: 0.7ms preprocess, 30.2ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS3\\yolov8n_train2\u001b[0m\n",
      "--- Training for method 'train' completed. Results saved to C:/Users/jenni/Desktop/YOLO_TRAIN_RESULTS3/yolov8n_train ---\n",
      "Could not retrieve validation metrics from training results.\n",
      "\n",
      "All training processes finished.\n"
     ]
    }
   ],
   "source": [
    "#조건1 양옆10%만 적용한 것의 학습 (aug2)\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import glob\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "BASE_AUGMENTED_PATH = \"C:/Users/jenni/Desktop/augmented_data3\" # 증강된 데이터의 최상위 경로 (poisson_harmonized 폴더가 이 아래에 있음)\n",
    "PROJECT_OUTPUT_PATH = \"C:/Users/jenni/Desktop/YOLO_TRAIN_RESULTS3\" # YOLO 학습 결과가 저장될 경로\n",
    "\n",
    "\n",
    "def convert_labelme_to_yolo_segmentation(json_dir, output_txt_dir, img_w, img_h, label_to_id_map):\n",
    "    \"\"\"\n",
    "    Labelme JSON 파일들을 YOLO segmentation .txt 파일로 변환합니다.\n",
    "    Args:\n",
    "        json_dir (str): Labelme JSON 파일들이 있는 디렉토리 경로.\n",
    "        output_txt_dir (str): 변환된 YOLO .txt 파일들을 저장할 디렉토리 경로.\n",
    "        img_w (int): 원본 이미지의 너비.\n",
    "        img_h (int): 원본 이미지의 높이.\n",
    "        label_to_id_map (dict): 클래스 이름(label)을 ID(숫자)로 매핑하는 딕셔너리.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_txt_dir, exist_ok=True)\n",
    "    json_files = glob.glob(os.path.join(json_dir, \"*.json\"))\n",
    "\n",
    "    for json_path in json_files:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        txt_filename = os.path.basename(json_path).replace(\".json\", \".txt\")\n",
    "        txt_path = os.path.join(output_txt_dir, txt_filename)\n",
    "\n",
    "        yolo_lines = []\n",
    "        for shape in data['shapes']:\n",
    "            label_name = shape['label']\n",
    "            if label_name in label_to_id_map:\n",
    "                class_id = label_to_id_map[label_name]\n",
    "                points = shape['points']\n",
    "                \n",
    "                normalized_points = []\n",
    "                for x, y in points:\n",
    "                    # 좌표를 이미지 크기에 맞춰 정규화\n",
    "                    normalized_points.append(f\"{(x / img_w):.6f}\")\n",
    "                    normalized_points.append(f\"{(y / img_h):.6f}\")\n",
    "                \n",
    "                yolo_lines.append(f\"{class_id} {' '.join(normalized_points)}\")\n",
    "            else:\n",
    "                print(f\"Warning: Class '{label_name}' not found in label_to_id_map. Skipping shape in {json_path}\")\n",
    "\n",
    "        if yolo_lines:\n",
    "            with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "                f.write('\\n'.join(yolo_lines))\n",
    "        # else:\n",
    "        #    # 객체가 없는 이미지도 빈 .txt 파일로 저장하는 것이 YOLO 학습에 더 안전합니다.\n",
    "        #    with open(txt_path, 'w') as f:\n",
    "        #        pass # 빈 파일 생성\n",
    "\n",
    "\n",
    "def create_data_yaml(dataset_base_path, yaml_output_path, class_names):\n",
    "    \"\"\"\n",
    "    YOLOv8 학습을 위한 data.yaml 파일을 생성합니다.\n",
    "    Args:\n",
    "        dataset_base_path (str): 증강된 이미지와 라벨이 있는 상위 디렉토리 (예: 'C:/Users/USER/Desktop/augmented_data1/poisson_harmonized')\n",
    "        yaml_output_path (str): 생성될 data.yaml 파일의 전체 경로.\n",
    "        class_names (list): 데이터셋의 모든 클래스 이름 목록.\n",
    "    \"\"\"\n",
    "    images_path = os.path.join(dataset_base_path, \"images\")\n",
    "    labels_path = os.path.join(dataset_base_path, \"labels_yolo\") # YOLO 형식의 라벨 폴더\n",
    "\n",
    "    # 경로를 상대 경로가 아닌 절대 경로로 지정하여 혼란 방지\n",
    "    train_img_path = images_path\n",
    "    val_img_path = images_path # 검증 데이터가 따로 없으면 train과 동일하게 설정\n",
    "\n",
    "    nc = len(class_names)\n",
    "    names = class_names\n",
    "\n",
    "    yaml_content = f\"\"\"\n",
    "path: {dataset_base_path}\n",
    "train: {os.path.relpath(train_img_path, dataset_base_path)} # path 기준으로 상대 경로\n",
    "val: {os.path.relpath(val_img_path, dataset_base_path)}\n",
    "\n",
    "nc: {nc}\n",
    "names: {names}\n",
    "\"\"\"\n",
    "    # 실제 YOLO 라벨 경로를 data.yaml이 참조하도록 labels 폴더의 이름을 변경하거나 (labels -> labels_yolo),\n",
    "    # data.yaml 내부에서 라벨 경로를 명시적으로 지정해야 합니다.\n",
    "    # 일반적으로 YOLO는 'images' 폴더와 같은 레벨의 'labels' 폴더를 기대합니다.\n",
    "    # 그래서 'labels_yolo'라는 폴더명을 가정했습니다.\n",
    "\n",
    "    with open(yaml_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(yaml_content.strip())\n",
    "    print(f\"'{yaml_output_path}' file created successfully.\")\n",
    "\n",
    "\n",
    "def train_yolo_for_method(method, epochs=50):\n",
    "    \"\"\"\n",
    "    특정 증강 방식에 대해 YOLOv8 모델을 학습시키고, 검증 결과를 도출합니다.\n",
    "    Args:\n",
    "        method (str): 증강 방식 이름 (예: 'poisson_harmonized').\n",
    "        epochs (int): 학습할 에포크 수.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting YOLOv8 training and validation for method: {method} ---\")\n",
    "\n",
    "    # 1. 각 증강 방식에 맞는 데이터셋 경로 설정\n",
    "    dataset_path = os.path.join(BASE_AUGMENTED_PATH, method)\n",
    "    images_path = os.path.join(dataset_path, \"images\")\n",
    "    labels_json_path = os.path.join(dataset_path, \"labels\") # Labelme JSON 라벨이 있는 곳\n",
    "    labels_yolo_path = os.path.join(dataset_path, \"labels_yolo\") # YOLO .txt 라벨을 저장할 곳\n",
    "\n",
    "    # 2. 클래스 정보를 미리 정의\n",
    "    label2id = {}\n",
    "    id2label = {}\n",
    "    try:\n",
    "        with open(\"classes.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            class_names_from_file = [line.strip() for line in f if line.strip()]\n",
    "        for i, name in enumerate(class_names_from_file):\n",
    "            label2id[name] = i\n",
    "            id2label[i] = name\n",
    "        print(f\"Loaded classes: {id2label}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'classes.txt' not found. Please ensure it's in the same directory or provide class info manually.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading classes.txt: {e}\")\n",
    "        return\n",
    "\n",
    "    # 이미지 크기를 추정하기 위해 첫 번째 이미지 파일을 읽습니다.\n",
    "    first_image_path = None\n",
    "    image_files = glob.glob(os.path.join(images_path, \"*.jpg\")) + glob.glob(os.path.join(images_path, \"*.png\"))\n",
    "    if image_files:\n",
    "        first_image_path = image_files[0]\n",
    "        img_temp = cv2.imread(first_image_path)\n",
    "        if img_temp is None:\n",
    "            print(f\"Error: Could not read image {first_image_path}. Check image file integrity.\")\n",
    "            return\n",
    "        img_h, img_w = img_temp.shape[:2]\n",
    "        print(f\"Detected image dimensions: W={img_w}, H={img_h}\")\n",
    "    else:\n",
    "        print(f\"Error: No images found in {images_path}. Cannot determine image dimensions for YOLO conversion.\")\n",
    "        return\n",
    "\n",
    "    # 3. Labelme JSON 라벨을 YOLO Segmentation .txt 형식으로 변환\n",
    "    print(f\"Converting Labelme JSON to YOLO segmentation .txt for {method}...\")\n",
    "    convert_labelme_to_yolo_segmentation(labels_json_path, labels_yolo_path, img_w, img_h, label2id)\n",
    "    print(\"Conversion complete.\")\n",
    "\n",
    "    # 4. data.yaml 파일 생성\n",
    "    yaml_path = os.path.join(dataset_path, \"data.yaml\")\n",
    "    create_data_yaml(dataset_path, yaml_path, list(id2label.values()))\n",
    "\n",
    "    # 5. 항상 동일한 사전 학습 모델로부터 학습 시작\n",
    "    model = YOLO(\"yolov8n-seg.pt\")\n",
    "    print(f\"Loaded YOLOv8n-seg model.\")\n",
    "\n",
    "    # 6. 모든 학습에 동일한 하이퍼파라미터 적용\n",
    "    try:\n",
    "        # 학습 수행\n",
    "        results = model.train(\n",
    "            data=yaml_path,\n",
    "            epochs=epochs,\n",
    "            imgsz=640,\n",
    "            name=f\"yolov8n_{method}\",\n",
    "            project=PROJECT_OUTPUT_PATH, # 결과 저장 최상위 경로\n",
    "            save=True,\n",
    "            resume=False\n",
    "        )\n",
    "        print(f\"--- Training for method '{method}' completed. Results saved to {PROJECT_OUTPUT_PATH}/yolov8n_{method} ---\")\n",
    "\n",
    "        # 7. 학습 완료 후 검증 수행 (자동으로 수행되지만 명시적으로 호출 가능)\n",
    "        # train 시에 이미 val이 포함되므로, 별도로 model.val()을 호출하지 않아도 결과는 results 객체에 포함됩니다.\n",
    "        # 하지만 특정 체크포인트를 사용하여 검증하고 싶다면 model.val()을 사용할 수 있습니다.\n",
    "        # 여기서는 학습 결과 객체에서 직접 metrics를 추출하는 방법을 보여줍니다.\n",
    "        \n",
    "        # results 객체에서 metrics 추출\n",
    "        # YOLOv8의 results 객체는 학습 결과 및 검증 지표를 포함합니다.\n",
    "        # 정확한 키 이름은 YOLOv8 버전에 따라 약간 다를 수 있으니,\n",
    "        # results 객체의 내용을 확인하는 것이 좋습니다 (예: print(results.metrics)).\n",
    "        # 일반적으로 segment 모델의 경우, 다음과 같은 키를 가집니다:\n",
    "        # metrics['metrics/precision(B)'] (Precision)\n",
    "        # metrics['metrics/recall(B)'] (Recall)\n",
    "        # metrics['metrics/mAP50(B)'] (mAP50)\n",
    "        # metrics['metrics/mAP50-95(B)'] (mAP50-95)\n",
    "        \n",
    "        if hasattr(results, 'metrics') and results.metrics:\n",
    "            print(\"\\n--- Validation Metrics ---\")\n",
    "            \n",
    "            # Segmentation 모델의 경우 mAP는 'mAP50', 'mAP50-95' 형태로 접근합니다.\n",
    "            # Precision, Recall, F1-score는 classification 또는 detection에서 더 흔하게 사용되지만,\n",
    "            # segment 결과에서도 'metrics/precision(B)', 'metrics/recall(B)'와 같은 키로 접근 가능합니다.\n",
    "            # F1-score는 Precision과 Recall로부터 계산해야 합니다.\n",
    "            \n",
    "            precision = results.metrics.get('metrics/precision(B)', 'N/A')\n",
    "            recall = results.metrics.get('metrics/recall(B)', 'N/A')\n",
    "            mAP50 = results.metrics.get('metrics/mAP50(B)', 'N/A')\n",
    "            mAP = results.metrics.get('metrics/mAP50-95(B)', 'N/A') # mAP (0.5 to 0.95 IOU)\n",
    "\n",
    "            # F1-score 계산\n",
    "            f1_score = 'N/A'\n",
    "            if precision != 'N/A' and recall != 'N/A' and (precision + recall) > 0:\n",
    "                f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "            print(f\"Precision: {precision:.4f}\")\n",
    "            print(f\"Recall: {recall:.4f}\")\n",
    "            print(f\"F1-Score: {f1_score:.4f}\")\n",
    "            print(f\"mAP50: {mAP50:.4f}\")\n",
    "            print(f\"mAP (50-95): {mAP:.4f}\")\n",
    "            print(\"------------------------\")\n",
    "        else:\n",
    "            print(\"Could not retrieve validation metrics from training results.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during YOLO training for method '{method}': {e}\")\n",
    "\n",
    "\n",
    "# --- 사용 예시 ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 학습을 시작할 증강 방식 이름 지정 (예: 이전 증강 스크립트에서 사용한 blending mode와 동일하게)\n",
    "    target_method = \"train\" # ⚠️ 여기에 실제로 사용한 증강 방식 폴더 이름을 입력하세요.\n",
    "\n",
    "    train_yolo_for_method(target_method, epochs=30)\n",
    "\n",
    "    print(\"\\nAll training processes finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc56daaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting YOLOv8 training and validation for method: train ---\n",
      "Loaded classes: {0: 'ac', 1: 'lc', 2: 'ph', 3: 'pc', 4: 'tc'}\n",
      "Detected image dimensions: W=1920, H=648\n",
      "Converting Labelme JSON to YOLO segmentation .txt for train...\n",
      "Conversion complete.\n",
      "'C:/Users/jenni/Desktop/augmented_data4\\train\\data.yaml' file created successfully.\n",
      "Loaded YOLOv8n-seg model.\n",
      "Ultralytics 8.3.152  Python-3.11.11 torch-2.7.1+cpu CPU (11th Gen Intel Core(TM) i7-11800H 2.30GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=C:/Users/jenni/Desktop/augmented_data4\\train\\data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n-seg.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8n_train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=C:/Users/jenni/Desktop/YOLO_TRAIN_RESULTS4, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=C:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS4\\yolov8n_train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=5\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1005055  ultralytics.nn.modules.head.Segment          [5, 32, 64, [64, 128, 256]]   \n",
      "YOLOv8n-seg summary: 151 layers, 3,264,591 parameters, 3,264,575 gradients, 12.1 GFLOPs\n",
      "\n",
      "Transferred 381/417 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 16.05.3 MB/s, size: 237.2 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\jenni\\Desktop\\augmented_data4\\train\\labels... 1032 images, 862 backgrounds, 0 corrupt: 100%|██████████| 1894/1894 [00:04<00:00, 425.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\jenni\\Desktop\\augmented_data4\\train\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 1098.0452.9 MB/s, size: 280.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\jenni\\Desktop\\augmented_data4\\train\\labels.cache... 1032 images, 862 backgrounds, 0 corrupt: 100%|██████████| 1894/1894 [00:00<?, ?it/s]\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to C:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS4\\yolov8n_train\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mC:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS4\\yolov8n_train\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50         0G       2.13      4.327      3.406      1.861         25        640: 100%|██████████| 119/119 [11:17<00:00,  5.69s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:50<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.467      0.137      0.104     0.0499      0.334     0.0855     0.0465      0.013\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50         0G      1.991      3.776      2.812       1.74         38        640: 100%|██████████| 119/119 [10:41<00:00,  5.39s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:49<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.297       0.21      0.127      0.061      0.274      0.162     0.0806     0.0266\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/50         0G       1.95      3.715      2.645      1.714         25        640: 100%|██████████| 119/119 [10:42<00:00,  5.40s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:48<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.597      0.207      0.128     0.0665      0.585      0.138     0.0841     0.0284\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/50         0G      1.932       3.65      2.545      1.682         26        640: 100%|██████████| 119/119 [10:26<00:00,  5.27s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:49<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.311      0.216      0.137     0.0718      0.301      0.159     0.0897     0.0282\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/50         0G      1.892      3.594       2.45      1.659         45        640: 100%|██████████| 119/119 [10:15<00:00,  5.17s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:48<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.405      0.242      0.173     0.0972      0.378      0.194      0.126     0.0422\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/50         0G      1.831      3.513      2.397      1.636         60        640: 100%|██████████| 119/119 [10:28<00:00,  5.28s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:49<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.304      0.265      0.172     0.0959      0.284      0.213      0.122     0.0431\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/50         0G      1.791      3.489      2.346      1.606         29        640: 100%|██████████| 119/119 [10:40<00:00,  5.38s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:48<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.383       0.28      0.179      0.102      0.371      0.218      0.135     0.0484\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/50         0G      1.782      3.466      2.307      1.589         33        640: 100%|██████████| 119/119 [10:25<00:00,  5.25s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:45<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.326      0.287      0.198      0.114       0.31      0.254      0.161     0.0606\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/50         0G      1.765      3.421      2.228      1.567         32        640: 100%|██████████| 119/119 [10:24<00:00,  5.24s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:45<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.317      0.283      0.184      0.103      0.304       0.25      0.152     0.0599\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/50         0G      1.728      3.423      2.204      1.564         58        640: 100%|██████████| 119/119 [10:25<00:00,  5.25s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:46<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.397      0.306      0.206       0.12      0.341      0.264      0.168     0.0624\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/50         0G      1.704      3.328      2.193      1.543         36        640: 100%|██████████| 119/119 [10:24<00:00,  5.25s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:40<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.349      0.329      0.218      0.131      0.338      0.299      0.191     0.0761\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/50         0G      1.699      3.331      2.183      1.541         44        640: 100%|██████████| 119/119 [10:22<00:00,  5.23s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:49<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.419      0.319      0.238      0.144      0.397      0.297      0.207     0.0838\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/50         0G      1.671      3.289      2.119      1.521         47        640: 100%|██████████| 119/119 [10:40<00:00,  5.38s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:41<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.372      0.334       0.24      0.145      0.361      0.299      0.205     0.0827\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/50         0G      1.669      3.282      2.108      1.526         47        640: 100%|██████████| 119/119 [10:22<00:00,  5.23s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:45<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.395      0.312      0.249      0.156      0.404      0.286      0.219     0.0916\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/50         0G      1.623      3.227       2.06      1.493         36        640: 100%|██████████| 119/119 [10:17<00:00,  5.19s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:43<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.381      0.345      0.258      0.159      0.352      0.313      0.218     0.0834\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/50         0G      1.616      3.228      2.059       1.49         56        640: 100%|██████████| 119/119 [10:34<00:00,  5.33s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:46<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.384      0.349      0.257      0.158      0.368      0.317      0.226     0.0921\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/50         0G      1.597      3.206       2.01      1.474         30        640: 100%|██████████| 119/119 [10:26<00:00,  5.27s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:44<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.405       0.36      0.271       0.17      0.404      0.327      0.244     0.0993\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/50         0G      1.594      3.202      2.016      1.471         56        640: 100%|██████████| 119/119 [10:27<00:00,  5.27s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:40<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.402      0.358      0.285       0.18      0.401      0.326      0.255      0.106\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/50         0G      1.568       3.16      1.971      1.461         43        640: 100%|██████████| 119/119 [10:26<00:00,  5.27s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:44<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.391      0.373      0.279      0.178       0.37      0.341      0.242     0.0969\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/50         0G      1.548      3.138      1.979       1.44         38        640: 100%|██████████| 119/119 [10:22<00:00,  5.23s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:45<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.416      0.364      0.292       0.19      0.402      0.343      0.255      0.105\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/50         0G      1.519      3.126      1.929      1.435         68        640: 100%|██████████| 119/119 [10:24<00:00,  5.25s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:49<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.394      0.366      0.302      0.196       0.39      0.352      0.275      0.119\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/50         0G      1.532      3.138       1.93      1.442         36        640: 100%|██████████| 119/119 [10:47<00:00,  5.44s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:49<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.442      0.367      0.309        0.2      0.433      0.343      0.273      0.112\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/50         0G      1.512      3.084      1.918      1.426         78        640: 100%|██████████| 119/119 [10:20<00:00,  5.21s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:41<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.416      0.381       0.31      0.203      0.385      0.346      0.263      0.108\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/50         0G      1.513      3.061      1.885      1.426         53        640: 100%|██████████| 119/119 [10:44<00:00,  5.42s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:43<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.415      0.375      0.306      0.201      0.396      0.346      0.259      0.107\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/50         0G       1.49      3.085      1.862      1.413         40        640: 100%|██████████| 119/119 [10:48<00:00,  5.45s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:42<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.458      0.368      0.322      0.215      0.435      0.352      0.291      0.124\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/50         0G      1.484      3.066      1.852      1.404         45        640: 100%|██████████| 119/119 [10:35<00:00,  5.34s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:37<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.406      0.397       0.32      0.208      0.383      0.366      0.284      0.119\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/50         0G      1.492       3.05      1.861      1.418         69        640: 100%|██████████| 119/119 [10:52<00:00,  5.49s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:43<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.429      0.401      0.349      0.233      0.419      0.379      0.314      0.132\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/50         0G      1.468      3.007      1.826      1.401         38        640: 100%|██████████| 119/119 [10:34<00:00,  5.33s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:43<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.454      0.386       0.35      0.233      0.426      0.351      0.303      0.128\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/50         0G      1.464      3.022       1.83      1.407         59        640: 100%|██████████| 119/119 [10:31<00:00,  5.30s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:40<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.445      0.397       0.34      0.227       0.43      0.374        0.3       0.13\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/50         0G      1.438      2.976      1.776      1.381         55        640: 100%|██████████| 119/119 [13:18<00:00,  6.71s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [02:19<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.448      0.423       0.36      0.243      0.439      0.392      0.321       0.14\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      31/50         0G      1.451      2.965      1.805      1.392         44        640: 100%|██████████| 119/119 [14:20<00:00,  7.23s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [02:16<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.461      0.417       0.37      0.254      0.443      0.386      0.327      0.141\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      32/50         0G      1.432      2.965       1.77      1.372         54        640: 100%|██████████| 119/119 [13:58<00:00,  7.05s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [02:17<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.463      0.422      0.369      0.253       0.45      0.394      0.334      0.144\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      33/50         0G      1.409      2.944      1.755      1.359         51        640: 100%|██████████| 119/119 [13:56<00:00,  7.03s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [3:38:12<00:00, 218.21s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.473      0.411      0.369      0.253      0.453      0.383      0.328      0.145\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      34/50         0G      1.384       2.91      1.727      1.349         47        640: 100%|██████████| 119/119 [09:50<00:00,  4.97s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:36<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.477      0.425      0.381      0.262      0.469      0.398      0.343      0.153\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      35/50         0G      1.384       2.91      1.721      1.352         42        640: 100%|██████████| 119/119 [10:26<00:00,  5.26s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:39<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.489      0.436      0.391      0.269      0.483      0.404      0.347      0.151\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      36/50         0G      1.369      2.887      1.692      1.346         36        640: 100%|██████████| 119/119 [10:38<00:00,  5.36s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:47<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.494      0.428      0.392      0.273      0.483      0.409      0.354      0.159\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      37/50         0G      1.364      2.871      1.672       1.34         27        640: 100%|██████████| 119/119 [10:48<00:00,  5.45s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:46<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.478      0.455      0.406      0.283      0.466      0.424      0.364      0.164\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      38/50         0G      1.358      2.893      1.691      1.336         58        640: 100%|██████████| 119/119 [10:54<00:00,  5.50s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:48<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.482      0.443      0.401      0.281      0.475      0.411      0.354      0.152\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      39/50         0G      1.345      2.876      1.668      1.334         29        640: 100%|██████████| 119/119 [10:32<00:00,  5.32s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:44<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.489      0.424      0.395      0.274      0.472      0.397      0.351      0.157\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      40/50         0G       1.35      2.851      1.658      1.332         34        640: 100%|██████████| 119/119 [10:59<00:00,  5.54s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:41<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.503      0.441      0.414      0.293      0.494      0.412      0.374      0.169\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "      41/50         0G      1.374      2.784      1.757       1.38         35        640: 100%|██████████| 119/119 [10:34<00:00,  5.34s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:40<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.483       0.44      0.402      0.281       0.47      0.414      0.357      0.158\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      42/50         0G      1.302      2.665      1.649      1.341         23        640: 100%|██████████| 119/119 [10:30<00:00,  5.30s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:44<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.506      0.438      0.416      0.294      0.483      0.415      0.374      0.168\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      43/50         0G       1.29       2.66      1.595      1.339         13        640: 100%|██████████| 119/119 [10:39<00:00,  5.38s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:38<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.505      0.437      0.422      0.299      0.487      0.411      0.376      0.172\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      44/50         0G      1.272      2.631      1.598      1.321         31        640: 100%|██████████| 119/119 [10:36<00:00,  5.35s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:47<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.516      0.459      0.433       0.31      0.498      0.435      0.392       0.18\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      45/50         0G       1.27      2.636       1.56      1.321          8        640: 100%|██████████| 119/119 [10:44<00:00,  5.42s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:46<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.511      0.452      0.431      0.308      0.488      0.424      0.387      0.176\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      46/50         0G      1.258      2.622      1.555      1.315         23        640: 100%|██████████| 119/119 [10:49<00:00,  5.46s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:40<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.523      0.453      0.438      0.314       0.51      0.421      0.394      0.182\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      47/50         0G      1.245      2.613      1.525      1.309         22        640: 100%|██████████| 119/119 [10:55<00:00,  5.51s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:40<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.518      0.472      0.447      0.321        0.5      0.433      0.397      0.183\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      48/50         0G       1.24      2.604      1.514      1.307         16        640: 100%|██████████| 119/119 [10:58<00:00,  5.53s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:45<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.546      0.456      0.449      0.325      0.516      0.428      0.402      0.187\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      49/50         0G      1.236      2.576      1.527      1.295         10        640: 100%|██████████| 119/119 [10:48<00:00,  5.45s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:41<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.528       0.46       0.45      0.325      0.513      0.431      0.401      0.186\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      50/50         0G      1.207      2.554      1.491      1.286         19        640: 100%|██████████| 119/119 [10:48<00:00,  5.45s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:40<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.537      0.462      0.453      0.327      0.524      0.434      0.409       0.19\n",
      "\n",
      "50 epochs completed in 14.148 hours.\n",
      "Optimizer stripped from C:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS4\\yolov8n_train\\weights\\last.pt, 6.7MB\n",
      "Optimizer stripped from C:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS4\\yolov8n_train\\weights\\best.pt, 6.7MB\n",
      "\n",
      "Validating C:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS4\\yolov8n_train\\weights\\best.pt...\n",
      "Ultralytics 8.3.152  Python-3.11.11 torch-2.7.1+cpu CPU (11th Gen Intel Core(TM) i7-11800H 2.30GHz)\n",
      "YOLOv8n-seg summary (fused): 85 layers, 3,259,039 parameters, 0 gradients, 12.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   2%|▏         | 1/60 [00:01<01:23,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   3%|▎         | 2/60 [00:02<01:27,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 60/60 [01:23<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1894       6232      0.537      0.462      0.453      0.327      0.526      0.433      0.409       0.19\n",
      "                    ac        767       1641      0.636      0.809      0.771      0.633      0.628      0.775      0.728      0.385\n",
      "                    lc        666       1214      0.602      0.699      0.685       0.54      0.576      0.648      0.606      0.242\n",
      "                    ph        771       1372      0.592      0.379      0.388      0.231      0.591      0.363      0.366      0.173\n",
      "                    pc        528        808      0.454      0.233      0.215      0.145      0.486      0.233       0.21      0.106\n",
      "                    tc        709       1197      0.401      0.188      0.205     0.0883      0.349      0.145      0.134     0.0449\n",
      "Speed: 0.7ms preprocess, 29.7ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS4\\yolov8n_train\u001b[0m\n",
      "--- Training for method 'train' completed. Results saved to C:/Users/jenni/Desktop/YOLO_TRAIN_RESULTS4/yolov8n_train ---\n",
      "Could not retrieve validation metrics from training results.\n",
      "\n",
      "All training processes finished.\n"
     ]
    }
   ],
   "source": [
    "#조건2 클래스 근처 20%만만 적용한 것+ 원본 파일 학습 (aug2)\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import glob\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "BASE_AUGMENTED_PATH = \"C:/Users/jenni/Desktop/augmented_data4\" # 증강된 데이터의 최상위 경로 (poisson_harmonized 폴더가 이 아래에 있음)\n",
    "PROJECT_OUTPUT_PATH = \"C:/Users/jenni/Desktop/YOLO_TRAIN_RESULTS4\" # YOLO 학습 결과가 저장될 경로\n",
    "\n",
    "\n",
    "def convert_labelme_to_yolo_segmentation(json_dir, output_txt_dir, img_w, img_h, label_to_id_map):\n",
    "    \"\"\"\n",
    "    Labelme JSON 파일들을 YOLO segmentation .txt 파일로 변환합니다.\n",
    "    Args:\n",
    "        json_dir (str): Labelme JSON 파일들이 있는 디렉토리 경로.\n",
    "        output_txt_dir (str): 변환된 YOLO .txt 파일들을 저장할 디렉토리 경로.\n",
    "        img_w (int): 원본 이미지의 너비.\n",
    "        img_h (int): 원본 이미지의 높이.\n",
    "        label_to_id_map (dict): 클래스 이름(label)을 ID(숫자)로 매핑하는 딕셔너리.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_txt_dir, exist_ok=True)\n",
    "    json_files = glob.glob(os.path.join(json_dir, \"*.json\"))\n",
    "\n",
    "    for json_path in json_files:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        txt_filename = os.path.basename(json_path).replace(\".json\", \".txt\")\n",
    "        txt_path = os.path.join(output_txt_dir, txt_filename)\n",
    "\n",
    "        yolo_lines = []\n",
    "        for shape in data['shapes']:\n",
    "            label_name = shape['label']\n",
    "            if label_name in label_to_id_map:\n",
    "                class_id = label_to_id_map[label_name]\n",
    "                points = shape['points']\n",
    "                \n",
    "                normalized_points = []\n",
    "                for x, y in points:\n",
    "                    # 좌표를 이미지 크기에 맞춰 정규화\n",
    "                    normalized_points.append(f\"{(x / img_w):.6f}\")\n",
    "                    normalized_points.append(f\"{(y / img_h):.6f}\")\n",
    "                \n",
    "                yolo_lines.append(f\"{class_id} {' '.join(normalized_points)}\")\n",
    "            else:\n",
    "                print(f\"Warning: Class '{label_name}' not found in label_to_id_map. Skipping shape in {json_path}\")\n",
    "\n",
    "        if yolo_lines:\n",
    "            with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "                f.write('\\n'.join(yolo_lines))\n",
    "        # else:\n",
    "        #    # 객체가 없는 이미지도 빈 .txt 파일로 저장하는 것이 YOLO 학습에 더 안전합니다.\n",
    "        #    with open(txt_path, 'w') as f:\n",
    "        #        pass # 빈 파일 생성\n",
    "\n",
    "\n",
    "def create_data_yaml(dataset_base_path, yaml_output_path, class_names):\n",
    "    \"\"\"\n",
    "    YOLOv8 학습을 위한 data.yaml 파일을 생성합니다.\n",
    "    Args:\n",
    "        dataset_base_path (str): 증강된 이미지와 라벨이 있는 상위 디렉토리 (예: 'C:/Users/USER/Desktop/augmented_data1/poisson_harmonized')\n",
    "        yaml_output_path (str): 생성될 data.yaml 파일의 전체 경로.\n",
    "        class_names (list): 데이터셋의 모든 클래스 이름 목록.\n",
    "    \"\"\"\n",
    "    images_path = os.path.join(dataset_base_path, \"images\")\n",
    "    labels_path = os.path.join(dataset_base_path, \"labels_yolo\") # YOLO 형식의 라벨 폴더\n",
    "\n",
    "    # 경로를 상대 경로가 아닌 절대 경로로 지정하여 혼란 방지\n",
    "    train_img_path = images_path\n",
    "    val_img_path = images_path # 검증 데이터가 따로 없으면 train과 동일하게 설정\n",
    "\n",
    "    nc = len(class_names)\n",
    "    names = class_names\n",
    "\n",
    "    yaml_content = f\"\"\"\n",
    "path: {dataset_base_path}\n",
    "train: {os.path.relpath(train_img_path, dataset_base_path)} # path 기준으로 상대 경로\n",
    "val: {os.path.relpath(val_img_path, dataset_base_path)}\n",
    "\n",
    "nc: {nc}\n",
    "names: {names}\n",
    "\"\"\"\n",
    "    # 실제 YOLO 라벨 경로를 data.yaml이 참조하도록 labels 폴더의 이름을 변경하거나 (labels -> labels_yolo),\n",
    "    # data.yaml 내부에서 라벨 경로를 명시적으로 지정해야 합니다.\n",
    "    # 일반적으로 YOLO는 'images' 폴더와 같은 레벨의 'labels' 폴더를 기대합니다.\n",
    "    # 그래서 'labels_yolo'라는 폴더명을 가정했습니다.\n",
    "\n",
    "    with open(yaml_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(yaml_content.strip())\n",
    "    print(f\"'{yaml_output_path}' file created successfully.\")\n",
    "\n",
    "\n",
    "def train_yolo_for_method(method, epochs=50):\n",
    "    \"\"\"\n",
    "    특정 증강 방식에 대해 YOLOv8 모델을 학습시키고, 검증 결과를 도출합니다.\n",
    "    Args:\n",
    "        method (str): 증강 방식 이름 (예: 'poisson_harmonized').\n",
    "        epochs (int): 학습할 에포크 수.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting YOLOv8 training and validation for method: {method} ---\")\n",
    "\n",
    "    # 1. 각 증강 방식에 맞는 데이터셋 경로 설정\n",
    "    dataset_path = os.path.join(BASE_AUGMENTED_PATH, method)\n",
    "    images_path = os.path.join(dataset_path, \"images\")\n",
    "    labels_json_path = os.path.join(dataset_path, \"labels\") # Labelme JSON 라벨이 있는 곳\n",
    "    labels_yolo_path = os.path.join(dataset_path, \"labels_yolo\") # YOLO .txt 라벨을 저장할 곳\n",
    "\n",
    "    # 2. 클래스 정보를 미리 정의\n",
    "    label2id = {}\n",
    "    id2label = {}\n",
    "    try:\n",
    "        with open(\"classes.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            class_names_from_file = [line.strip() for line in f if line.strip()]\n",
    "        for i, name in enumerate(class_names_from_file):\n",
    "            label2id[name] = i\n",
    "            id2label[i] = name\n",
    "        print(f\"Loaded classes: {id2label}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'classes.txt' not found. Please ensure it's in the same directory or provide class info manually.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading classes.txt: {e}\")\n",
    "        return\n",
    "\n",
    "    # 이미지 크기를 추정하기 위해 첫 번째 이미지 파일을 읽습니다.\n",
    "    first_image_path = None\n",
    "    image_files = glob.glob(os.path.join(images_path, \"*.jpg\")) + glob.glob(os.path.join(images_path, \"*.png\"))\n",
    "    if image_files:\n",
    "        first_image_path = image_files[0]\n",
    "        img_temp = cv2.imread(first_image_path)\n",
    "        if img_temp is None:\n",
    "            print(f\"Error: Could not read image {first_image_path}. Check image file integrity.\")\n",
    "            return\n",
    "        img_h, img_w = img_temp.shape[:2]\n",
    "        print(f\"Detected image dimensions: W={img_w}, H={img_h}\")\n",
    "    else:\n",
    "        print(f\"Error: No images found in {images_path}. Cannot determine image dimensions for YOLO conversion.\")\n",
    "        return\n",
    "\n",
    "    # 3. Labelme JSON 라벨을 YOLO Segmentation .txt 형식으로 변환\n",
    "    print(f\"Converting Labelme JSON to YOLO segmentation .txt for {method}...\")\n",
    "    convert_labelme_to_yolo_segmentation(labels_json_path, labels_yolo_path, img_w, img_h, label2id)\n",
    "    print(\"Conversion complete.\")\n",
    "\n",
    "    # 4. data.yaml 파일 생성\n",
    "    yaml_path = os.path.join(dataset_path, \"data.yaml\")\n",
    "    create_data_yaml(dataset_path, yaml_path, list(id2label.values()))\n",
    "\n",
    "    # 5. 항상 동일한 사전 학습 모델로부터 학습 시작\n",
    "    model = YOLO(\"yolov8n-seg.pt\")\n",
    "    print(f\"Loaded YOLOv8n-seg model.\")\n",
    "\n",
    "    # 6. 모든 학습에 동일한 하이퍼파라미터 적용\n",
    "    try:\n",
    "        # 학습 수행\n",
    "        results = model.train(\n",
    "            data=yaml_path,\n",
    "            epochs=epochs,\n",
    "            imgsz=640,\n",
    "            name=f\"yolov8n_{method}\",\n",
    "            project=PROJECT_OUTPUT_PATH, # 결과 저장 최상위 경로\n",
    "            save=True,\n",
    "            resume=False\n",
    "        )\n",
    "        print(f\"--- Training for method '{method}' completed. Results saved to {PROJECT_OUTPUT_PATH}/yolov8n_{method} ---\")\n",
    "\n",
    "        # 7. 학습 완료 후 검증 수행 (자동으로 수행되지만 명시적으로 호출 가능)\n",
    "        # train 시에 이미 val이 포함되므로, 별도로 model.val()을 호출하지 않아도 결과는 results 객체에 포함됩니다.\n",
    "        # 하지만 특정 체크포인트를 사용하여 검증하고 싶다면 model.val()을 사용할 수 있습니다.\n",
    "        # 여기서는 학습 결과 객체에서 직접 metrics를 추출하는 방법을 보여줍니다.\n",
    "        \n",
    "        # results 객체에서 metrics 추출\n",
    "        # YOLOv8의 results 객체는 학습 결과 및 검증 지표를 포함합니다.\n",
    "        # 정확한 키 이름은 YOLOv8 버전에 따라 약간 다를 수 있으니,\n",
    "        # results 객체의 내용을 확인하는 것이 좋습니다 (예: print(results.metrics)).\n",
    "        # 일반적으로 segment 모델의 경우, 다음과 같은 키를 가집니다:\n",
    "        # metrics['metrics/precision(B)'] (Precision)\n",
    "        # metrics['metrics/recall(B)'] (Recall)\n",
    "        # metrics['metrics/mAP50(B)'] (mAP50)\n",
    "        # metrics['metrics/mAP50-95(B)'] (mAP50-95)\n",
    "        \n",
    "        if hasattr(results, 'metrics') and results.metrics:\n",
    "            print(\"\\n--- Validation Metrics ---\")\n",
    "            \n",
    "            # Segmentation 모델의 경우 mAP는 'mAP50', 'mAP50-95' 형태로 접근합니다.\n",
    "            # Precision, Recall, F1-score는 classification 또는 detection에서 더 흔하게 사용되지만,\n",
    "            # segment 결과에서도 'metrics/precision(B)', 'metrics/recall(B)'와 같은 키로 접근 가능합니다.\n",
    "            # F1-score는 Precision과 Recall로부터 계산해야 합니다.\n",
    "            \n",
    "            precision = results.metrics.get('metrics/precision(B)', 'N/A')\n",
    "            recall = results.metrics.get('metrics/recall(B)', 'N/A')\n",
    "            mAP50 = results.metrics.get('metrics/mAP50(B)', 'N/A')\n",
    "            mAP = results.metrics.get('metrics/mAP50-95(B)', 'N/A') # mAP (0.5 to 0.95 IOU)\n",
    "\n",
    "            # F1-score 계산\n",
    "            f1_score = 'N/A'\n",
    "            if precision != 'N/A' and recall != 'N/A' and (precision + recall) > 0:\n",
    "                f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "            print(f\"Precision: {precision:.4f}\")\n",
    "            print(f\"Recall: {recall:.4f}\")\n",
    "            print(f\"F1-Score: {f1_score:.4f}\")\n",
    "            print(f\"mAP50: {mAP50:.4f}\")\n",
    "            print(f\"mAP (50-95): {mAP:.4f}\")\n",
    "            print(\"------------------------\")\n",
    "        else:\n",
    "            print(\"Could not retrieve validation metrics from training results.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during YOLO training for method '{method}': {e}\")\n",
    "\n",
    "\n",
    "# --- 사용 예시 ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 학습을 시작할 증강 방식 이름 지정 (예: 이전 증강 스크립트에서 사용한 blending mode와 동일하게)\n",
    "    target_method = \"train\" # ⚠️ 여기에 실제로 사용한 증강 방식 폴더 이름을 입력하세요.\n",
    "\n",
    "    train_yolo_for_method(target_method, epochs=50)\n",
    "\n",
    "    print(\"\\nAll training processes finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c9cd90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting YOLOv8 training for dataset: train ---\n",
      "Loaded classes: {0: 'ac', 1: 'lc', 2: 'ph', 3: 'pc', 4: 'tc'}\n",
      "Detected image dimensions from training data: W=1920, H=648\n",
      "\n",
      "Converting training labels (JSON -> YOLO .txt)...\n",
      "Warning: No JSON files found in C:/Users/jenni/Desktop/augmented_data2\\train\\train\\labels. Skipping conversion for this directory.\n",
      "Converting validation labels (JSON -> YOLO .txt)...\n",
      "Label conversion complete.\n",
      "\n",
      "Creating 'data.yaml' configuration file...\n",
      "'C:/Users/jenni/Desktop/augmented_data2\\train\\data.yaml' file created successfully.\n",
      "\n",
      "Loading YOLOv8 model and starting training...\n",
      "Ultralytics 8.3.152  Python-3.11.11 torch-2.7.1+cpu CPU (11th Gen Intel Core(TM) i7-11800H 2.30GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=C:/Users/jenni/Desktop/augmented_data2\\train\\data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n-seg.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8n_seg_train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=C:/Users/jenni/Desktop/YOLO_TRAIN_RESULTS2, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=C:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS2\\yolov8n_seg_train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=5\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1005055  ultralytics.nn.modules.head.Segment          [5, 32, 64, [64, 128, 256]]   \n",
      "YOLOv8n-seg summary: 151 layers, 3,264,591 parameters, 3,264,575 gradients, 12.1 GFLOPs\n",
      "\n",
      "Transferred 381/417 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 37.111.8 MB/s, size: 354.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\jenni\\Desktop\\augmented_data2\\train\\train\\labels... 1032 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1032/1032 [00:02<00:00, 495.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\jenni\\Desktop\\augmented_data2\\train\\train\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 16.22.0 MB/s, size: 157.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\jenni\\Desktop\\augmented_data2\\train\\val\\labels... 0 images, 216 backgrounds, 0 corrupt: 100%|██████████| 216/216 [00:00<00:00, 1019.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING \u001b[34m\u001b[1mval: \u001b[0mNo labels found in C:\\Users\\jenni\\Desktop\\augmented_data2\\train\\val\\labels.cache. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\jenni\\Desktop\\augmented_data2\\train\\val\\labels.cache\n",
      "WARNING Labels are missing or empty in C:\\Users\\jenni\\Desktop\\augmented_data2\\train\\val\\labels.cache, training may not work correctly. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n",
      "Plotting labels to C:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS2\\yolov8n_seg_train\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mC:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS2\\yolov8n_seg_train\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      2.212      4.492       3.21      1.934        112        640: 100%|██████████| 65/65 [07:08<00:00,  6.58s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:13<00:00,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        216          0          0          0          0          0          0          0          0          0\n",
      "WARNING no labels found in segment set, can not compute metrics without labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:767: RuntimeWarning: Mean of empty slice.\n",
      "  i = smooth(f1_curve.mean(0), 0.1).argmax()  # max F1 index\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\numpy\\_core\\_methods.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G      1.926      3.761      2.386      1.702        120        640: 100%|██████████| 65/65 [07:18<00:00,  6.75s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:13<00:00,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        216          0          0          0          0          0          0          0          0          0\n",
      "WARNING no labels found in segment set, can not compute metrics without labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:767: RuntimeWarning: Mean of empty slice.\n",
      "  i = smooth(f1_curve.mean(0), 0.1).argmax()  # max F1 index\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\numpy\\_core\\_methods.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G      1.857      3.592      2.218      1.655        111        640: 100%|██████████| 65/65 [07:07<00:00,  6.58s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:13<00:00,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        216          0          0          0          0          0          0          0          0          0\n",
      "WARNING no labels found in segment set, can not compute metrics without labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:767: RuntimeWarning: Mean of empty slice.\n",
      "  i = smooth(f1_curve.mean(0), 0.1).argmax()  # max F1 index\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\numpy\\_core\\_methods.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      1.783      3.513       2.09        1.6        126        640: 100%|██████████| 65/65 [06:59<00:00,  6.45s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:15<00:00,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        216          0          0          0          0          0          0          0          0          0\n",
      "WARNING no labels found in segment set, can not compute metrics without labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:767: RuntimeWarning: Mean of empty slice.\n",
      "  i = smooth(f1_curve.mean(0), 0.1).argmax()  # max F1 index\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\numpy\\_core\\_methods.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G      1.702        3.4      1.987      1.555        102        640: 100%|██████████| 65/65 [07:29<00:00,  6.92s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:14<00:00,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        216          0          0          0          0          0          0          0          0          0\n",
      "WARNING no labels found in segment set, can not compute metrics without labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:767: RuntimeWarning: Mean of empty slice.\n",
      "  i = smooth(f1_curve.mean(0), 0.1).argmax()  # max F1 index\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\numpy\\_core\\_methods.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.621 hours.\n",
      "Optimizer stripped from C:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS2\\yolov8n_seg_train\\weights\\last.pt, 6.7MB\n",
      "Optimizer stripped from C:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS2\\yolov8n_seg_train\\weights\\best.pt, 6.7MB\n",
      "\n",
      "Validating C:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS2\\yolov8n_seg_train\\weights\\best.pt...\n",
      "Ultralytics 8.3.152  Python-3.11.11 torch-2.7.1+cpu CPU (11th Gen Intel Core(TM) i7-11800H 2.30GHz)\n",
      "YOLOv8n-seg summary (fused): 85 layers, 3,259,039 parameters, 0 gradients, 12.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):  14%|█▍        | 1/7 [00:02<00:12,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):  29%|██▊       | 2/7 [00:04<00:10,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:13<00:00,  1.98s/it]\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:585: RuntimeWarning: Mean of empty slice.\n",
      "  ax.plot(px, py.mean(1), linewidth=3, color=\"blue\", label=f\"all classes {ap[:, 0].mean():.3f} mAP@0.5\")\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\numpy\\_core\\_methods.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:630: RuntimeWarning: Mean of empty slice.\n",
      "  y = smooth(py.mean(0), 0.1)\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\numpy\\_core\\_methods.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:630: RuntimeWarning: Mean of empty slice.\n",
      "  y = smooth(py.mean(0), 0.1)\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\numpy\\_core\\_methods.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:630: RuntimeWarning: Mean of empty slice.\n",
      "  y = smooth(py.mean(0), 0.1)\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\numpy\\_core\\_methods.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:767: RuntimeWarning: Mean of empty slice.\n",
      "  i = smooth(f1_curve.mean(0), 0.1).argmax()  # max F1 index\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:585: RuntimeWarning: Mean of empty slice.\n",
      "  ax.plot(px, py.mean(1), linewidth=3, color=\"blue\", label=f\"all classes {ap[:, 0].mean():.3f} mAP@0.5\")\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\numpy\\_core\\_methods.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:630: RuntimeWarning: Mean of empty slice.\n",
      "  y = smooth(py.mean(0), 0.1)\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\numpy\\_core\\_methods.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:630: RuntimeWarning: Mean of empty slice.\n",
      "  y = smooth(py.mean(0), 0.1)\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\numpy\\_core\\_methods.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:630: RuntimeWarning: Mean of empty slice.\n",
      "  y = smooth(py.mean(0), 0.1)\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\numpy\\_core\\_methods.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:767: RuntimeWarning: Mean of empty slice.\n",
      "  i = smooth(f1_curve.mean(0), 0.1).argmax()  # max F1 index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        216          0          0          0          0          0          0          0          0          0\n",
      "WARNING no labels found in segment set, can not compute metrics without labels\n",
      "Speed: 0.7ms preprocess, 36.8ms inference, 0.0ms loss, 5.4ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS2\\yolov8n_seg_train\u001b[0m\n",
      "--- Training for dataset 'train' completed. ---\n",
      "Could not retrieve validation metrics from training results.\n",
      "\n",
      "All training processes finished.\n"
     ]
    }
   ],
   "source": [
    "# -*- copy-paste만 적용\n",
    "\"\"\"YOLOv8 학습 및 검증 스크립트 (Train/Val 분리)\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1GTwD00wWTckTswWBJxy8ye1o3Mllj-3w\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import glob\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# --- 경로 설정 ---\n",
    "# 데이터셋의 상위 경로 (예: 'augmented_data2' 폴더가 있는 위치)\n",
    "BASE_DATASET_PATH = \"C:/Users/jenni/Desktop/augmented_data2\"\n",
    "# YOLO 학습 결과가 저장될 경로\n",
    "PROJECT_OUTPUT_PATH = \"C:/Users/jenni/Desktop/YOLO_TRAIN_RESULTS2\"\n",
    "\n",
    "\n",
    "def convert_labelme_to_yolo_segmentation(json_dir, output_txt_dir, img_w, img_h, label_to_id_map):\n",
    "    \"\"\"\n",
    "    Labelme JSON 파일들을 YOLO segmentation .txt 파일로 변환합니다.\n",
    "    Args:\n",
    "        json_dir (str): Labelme JSON 파일들이 있는 디렉토리 경로.\n",
    "        output_txt_dir (str): 변환된 YOLO .txt 파일들을 저장할 디렉토리 경로.\n",
    "        img_w (int): 원본 이미지의 너비.\n",
    "        img_h (int): 원본 이미지의 높이.\n",
    "        label_to_id_map (dict): 클래스 이름(label)을 ID(숫자)로 매핑하는 딕셔너리.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_txt_dir, exist_ok=True)\n",
    "    json_files = glob.glob(os.path.join(json_dir, \"*.json\"))\n",
    "\n",
    "    if not json_files:\n",
    "        print(f\"Warning: No JSON files found in {json_dir}. Skipping conversion for this directory.\")\n",
    "        return\n",
    "\n",
    "    for json_path in json_files:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        txt_filename = os.path.basename(json_path).replace(\".json\", \".txt\")\n",
    "        txt_path = os.path.join(output_txt_dir, txt_filename)\n",
    "\n",
    "        yolo_lines = []\n",
    "        for shape in data.get('shapes', []):\n",
    "            label_name = shape.get('label')\n",
    "            if label_name in label_to_id_map:\n",
    "                class_id = label_to_id_map[label_name]\n",
    "                points = shape.get('points', [])\n",
    "\n",
    "                normalized_points = []\n",
    "                for x, y in points:\n",
    "                    # 좌표를 이미지 크기에 맞춰 정규화\n",
    "                    normalized_points.append(f\"{(x / img_w):.6f}\")\n",
    "                    normalized_points.append(f\"{(y / img_h):.6f}\")\n",
    "\n",
    "                if normalized_points:\n",
    "                    yolo_lines.append(f\"{class_id} {' '.join(normalized_points)}\")\n",
    "            else:\n",
    "                print(f\"Warning: Class '{label_name}' not found in label_to_id_map. Skipping shape in {json_path}\")\n",
    "\n",
    "        # 객체가 있든 없든 항상 .txt 파일을 생성하여 학습 안정성을 높입니다.\n",
    "        with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "            if yolo_lines:\n",
    "                f.write('\\n'.join(yolo_lines))\n",
    "            else:\n",
    "                pass # 객체가 없는 경우 빈 파일 생성\n",
    "\n",
    "\n",
    "def create_data_yaml(dataset_base_path, yaml_output_path, class_names):\n",
    "    \"\"\"\n",
    "    YOLOv8 학습을 위한 data.yaml 파일을 생성합니다. (train/val 분리 구조)\n",
    "    Args:\n",
    "        dataset_base_path (str): 'train', 'val' 폴더가 있는 상위 디렉토리.\n",
    "        yaml_output_path (str): 생성될 data.yaml 파일의 전체 경로.\n",
    "        class_names (list): 데이터셋의 모든 클래스 이름 목록.\n",
    "    \"\"\"\n",
    "    # YOLOv8은 data.yaml에 지정된 'path'를 기준으로 train/val 경로를 찾습니다.\n",
    "    # 경로는 호환성을 위해 슬래시(/)로 변환합니다.\n",
    "    path_str = dataset_base_path.replace(os.sep, '/')\n",
    "    train_images_rel_path = 'train/images'\n",
    "    val_images_rel_path = 'val/images'\n",
    "\n",
    "    nc = len(class_names)\n",
    "\n",
    "    yaml_content = f\"\"\"\n",
    "# YOLOv8 Dataset Configuration File\n",
    "# Paths are relative to the 'path' directory.\n",
    "\n",
    "path: {path_str}\n",
    "train: {train_images_rel_path}\n",
    "val: {val_images_rel_path}\n",
    "\n",
    "# Number of classes and names\n",
    "nc: {nc}\n",
    "names: {class_names}\n",
    "\"\"\"\n",
    "    with open(yaml_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(yaml_content.strip())\n",
    "    print(f\"'{yaml_output_path}' file created successfully.\")\n",
    "\n",
    "\n",
    "def run_yolo_training(dataset_name, epochs=50):\n",
    "    \"\"\"\n",
    "    'train'/'val' 구조의 데이터셋으로 YOLOv8 모델을 학습하고 검증합니다.\n",
    "    Args:\n",
    "        dataset_name (str): 'train', 'val' 폴더를 포함하는 데이터셋 폴더 이름.\n",
    "                              (BASE_DATASET_PATH 아래에 위치해야 함)\n",
    "        epochs (int): 학습할 에포크 수.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting YOLOv8 training for dataset: {dataset_name} ---\")\n",
    "\n",
    "    # 1. 경로 설정\n",
    "    dataset_path = os.path.join(BASE_DATASET_PATH, dataset_name)\n",
    "    train_path = os.path.join(dataset_path, \"train\")\n",
    "    val_path = os.path.join(dataset_path, \"val\")\n",
    "\n",
    "    # 필요한 폴더 존재 여부 확인\n",
    "    if not (os.path.isdir(train_path) and os.path.isdir(val_path)):\n",
    "        print(f\"Error: 'train' and/or 'val' directory not found in '{dataset_path}'\")\n",
    "        return\n",
    "\n",
    "    train_images_path = os.path.join(train_path, \"images\")\n",
    "    train_labels_json_path = os.path.join(train_path, \"labels\")\n",
    "    train_labels_yolo_path = os.path.join(train_path, \"labels_yolo\") # 변환된 YOLO 라벨 저장 위치\n",
    "\n",
    "    val_labels_json_path = os.path.join(val_path, \"labels\")\n",
    "    val_labels_yolo_path = os.path.join(val_path, \"labels_yolo\") # 변환된 YOLO 라벨 저장 위치\n",
    "\n",
    "    # 2. 클래스 정보 로드\n",
    "    label2id = {}\n",
    "    id2label = {}\n",
    "    try:\n",
    "        with open(\"classes.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            class_names_from_file = [line.strip() for line in f if line.strip()]\n",
    "        for i, name in enumerate(class_names_from_file):\n",
    "            label2id[name] = i\n",
    "            id2label[i] = name\n",
    "        print(f\"Loaded classes: {id2label}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'classes.txt' not found. Please place it in the same directory as the script.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading classes.txt: {e}\")\n",
    "        return\n",
    "\n",
    "    # 3. 이미지 크기 확인 (학습 이미지 기준)\n",
    "    image_files = glob.glob(os.path.join(train_images_path, \"*.jpg\")) + glob.glob(os.path.join(train_images_path, \"*.png\"))\n",
    "    if not image_files:\n",
    "        print(f\"Error: No images found in {train_images_path}. Cannot proceed.\")\n",
    "        return\n",
    "\n",
    "    first_image_path = image_files[0]\n",
    "    img_temp = cv2.imread(first_image_path)\n",
    "    if img_temp is None:\n",
    "        print(f\"Error: Could not read image {first_image_path}. Check file integrity.\")\n",
    "        return\n",
    "    img_h, img_w = img_temp.shape[:2]\n",
    "    print(f\"Detected image dimensions from training data: W={img_w}, H={img_h}\")\n",
    "\n",
    "    # 4. Labelme JSON을 YOLO .txt로 변환 (Train & Val)\n",
    "    print(\"\\nConverting training labels (JSON -> YOLO .txt)...\")\n",
    "    convert_labelme_to_yolo_segmentation(train_labels_json_path, train_labels_yolo_path, img_w, img_h, label2id)\n",
    "    print(\"Converting validation labels (JSON -> YOLO .txt)...\")\n",
    "    convert_labelme_to_yolo_segmentation(val_labels_json_path, val_labels_yolo_path, img_w, img_h, label2id)\n",
    "    print(\"Label conversion complete.\")\n",
    "\n",
    "    # 5. data.yaml 파일 생성\n",
    "    print(\"\\nCreating 'data.yaml' configuration file...\")\n",
    "    yaml_path = os.path.join(dataset_path, \"data.yaml\")\n",
    "    create_data_yaml(dataset_path, yaml_path, list(id2label.values()))\n",
    "\n",
    "    # 6. YOLOv8 모델 학습\n",
    "    print(\"\\nLoading YOLOv8 model and starting training...\")\n",
    "    model = YOLO(\"yolov8n-seg.pt\")\n",
    "\n",
    "    try:\n",
    "        results = model.train(\n",
    "            data=yaml_path,\n",
    "            epochs=epochs,\n",
    "            imgsz=640,\n",
    "            name=f\"yolov8n_seg_{dataset_name}\", # 결과 폴더 이름\n",
    "            project=PROJECT_OUTPUT_PATH,\n",
    "            save=True,      # 모델 저장\n",
    "            resume=False    # 이어서 학습 안 함\n",
    "        )\n",
    "        print(f\"--- Training for dataset '{dataset_name}' completed. ---\")\n",
    "\n",
    "        # 7. 검증 결과 출력 (학습 시 자동으로 검증이 수행됩니다)\n",
    "        if hasattr(results, 'metrics') and results.metrics:\n",
    "            print(\"\\n--- Validation Metrics ---\")\n",
    "            precision = results.metrics.get('metrics/precision(B)', 0.0)\n",
    "            recall = results.metrics.get('metrics/recall(B)', 0.0)\n",
    "            mAP50_B = results.metrics.get('metrics/mAP50(B)', 0.0)\n",
    "            mAP_B = results.metrics.get('metrics/mAP50-95(B)', 0.0)\n",
    "            mAP50_M = results.metrics.get('metrics/mAP50(M)', 0.0)\n",
    "            mAP_M = results.metrics.get('metrics/mAP50-95(M)', 0.0)\n",
    "\n",
    "            f1_score = 0.0\n",
    "            if (precision + recall) > 0:\n",
    "                f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "            print(f\"Precision (Box): {precision:.4f}\")\n",
    "            print(f\"Recall (Box): {recall:.4f}\")\n",
    "            print(f\"F1-Score (Box): {f1_score:.4f}\")\n",
    "            print(\"-\" * 20)\n",
    "            print(f\"mAP@.50 (Box): {mAP50_B:.4f}\")\n",
    "            print(f\"mAP@.50-.95 (Box): {mAP_B:.4f}\")\n",
    "            print(\"-\" * 20)\n",
    "            print(f\"mAP@.50 (Mask): {mAP50_M:.4f}\")\n",
    "            print(f\"mAP@.50-.95 (Mask): {mAP_M:.4f}\")\n",
    "            print(\"------------------------\")\n",
    "        else:\n",
    "            print(\"Could not retrieve validation metrics from training results.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during YOLO training for dataset '{dataset_name}': {e}\")\n",
    "\n",
    "\n",
    "# --- 스크립트 실행 ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 학습을 진행할 데이터셋 폴더 이름을 지정합니다.\n",
    "    # 예: BASE_DATASET_PATH가 \"C:/.../augmented_data3\"이고,\n",
    "    # 그 안에 \"my_dataset\" 폴더가 있으며,\n",
    "    # \"my_dataset\" 안에 \"train\", \"val\" 폴더가 있다면 \"my_dataset\"을 입력합니다.\n",
    "    # 제공해주신 스크린샷 기준으로 폴더 이름이 없다면 새로 만드셔야 합니다.\n",
    "    # 예를 들어, augmented_data3 안에 'dataset1' 폴더를 만들고 그 안에 train, val을 넣었다면\n",
    "    # target_dataset = \"dataset1\" 로 설정합니다.\n",
    "    target_dataset = \"train\" # ⚠️ 실제 데이터셋 폴더 이름으로 변경해주세요.\n",
    "\n",
    "    # 에포크 수 설정\n",
    "    num_epochs = 5\n",
    "\n",
    "    run_yolo_training(target_dataset, epochs=num_epochs)\n",
    "\n",
    "    print(\"\\nAll training processes finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fdf7748-5f99-43d9-8377-4ae5a9db64a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting YOLOv8 training and validation for method: train ---\n",
      "Using device: cpu\n",
      "Warning: No GPU found. Training will be significantly slower on CPU. Ensure PyTorch with CUDA is installed correctly if you intend to use a GPU.\n",
      "Loaded classes: {0: 'ac', 1: 'lc', 2: 'ph', 3: 'pc', 4: 'tc'}\n",
      "Detected image dimensions: W=1920, H=648\n",
      "Converting Labelme JSON to YOLO segmentation .txt for train...\n",
      "Conversion complete.\n",
      "'C:/Users/jenni/Desktop/augmented_data1\\train\\data.yaml' file created successfully.\n",
      "Loaded YOLOv8n-seg model.\n",
      "Ultralytics 8.3.152  Python-3.11.11 torch-2.7.1+cpu CPU (11th Gen Intel Core(TM) i7-11800H 2.30GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=C:/Users/jenni/Desktop/augmented_data1\\train\\data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n-seg.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8n_train2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=C:/Users/jenni/Desktop/YOLO_TRAIN_RESULTS, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=C:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS\\yolov8n_train2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=5\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1005055  ultralytics.nn.modules.head.Segment          [5, 32, 64, [64, 128, 256]]   \n",
      "YOLOv8n-seg summary: 151 layers, 3,264,591 parameters, 3,264,575 gradients, 12.1 GFLOPs\n",
      "\n",
      "Transferred 381/417 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 1493.5371.9 MB/s, size: 359.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\jenni\\Desktop\\augmented_data1\\train\\labels.cache... 991 images, 9 backgrounds, 0 corrupt: 100%|██████████| 1000/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 1755.1391.6 MB/s, size: 323.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\jenni\\Desktop\\augmented_data1\\train\\labels.cache... 991 images, 9 backgrounds, 0 corrupt: 100%|██████████| 1000/1000 [00:00<?, ?it/s]\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to C:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS\\yolov8n_train2\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mC:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS\\yolov8n_train2\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50         0G      3.135      4.787      4.444      2.288         47        640: 100%|██████████| 63/63 [06:05<00:00,  5.80s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:17<00:00,  2.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931     0.0018      0.219     0.0127    0.00554   0.000881     0.0467     0.0117    0.00472\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50         0G      2.861      4.239      3.745      2.086         53        640: 100%|██████████| 63/63 [07:09<00:00,  6.82s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:23<00:00,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.532     0.0348     0.0366     0.0149      0.529     0.0319     0.0313     0.0114\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/50         0G      2.797      4.176      3.522      2.044         30        640: 100%|██████████| 63/63 [07:09<00:00,  6.82s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:22<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931     0.0771     0.0856     0.0415     0.0171     0.0862     0.0645     0.0347     0.0138\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/50         0G      2.758      4.091      3.354          2         56        640: 100%|██████████| 63/63 [06:53<00:00,  6.57s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:05<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.311     0.0745     0.0509     0.0214      0.302     0.0667     0.0425     0.0157\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/50         0G      2.716      4.068       3.27      1.972         58        640: 100%|██████████| 63/63 [05:45<00:00,  5.49s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:02<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.187     0.0659     0.0492     0.0191      0.119     0.0548     0.0388     0.0139\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/50         0G      2.655      4.049      3.223       1.96         42        640: 100%|██████████| 63/63 [05:48<00:00,  5.54s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:04<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.164      0.104     0.0675     0.0286      0.155     0.0949      0.059     0.0229\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/50         0G      2.605      3.994      3.098      1.908         54        640: 100%|██████████| 63/63 [05:59<00:00,  5.70s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:03<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.196      0.108     0.0639     0.0269      0.193     0.0931     0.0563     0.0189\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/50         0G      2.579      3.988      3.038      1.889         42        640: 100%|██████████| 63/63 [05:44<00:00,  5.47s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:00<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.473     0.0974     0.0838     0.0367      0.471     0.0805     0.0597     0.0227\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/50         0G       2.58      3.938      2.991      1.881         57        640: 100%|██████████| 63/63 [05:47<00:00,  5.52s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:01<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.236      0.125     0.0829     0.0364      0.197      0.102     0.0629      0.024\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/50         0G      2.504      3.895      2.913      1.859         52        640: 100%|██████████| 63/63 [05:46<00:00,  5.51s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:03<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.267      0.126      0.107     0.0467      0.208     0.0995     0.0785     0.0283\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/50         0G      2.472      3.886      2.871      1.846         48        640: 100%|██████████| 63/63 [05:50<00:00,  5.56s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:59<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931       0.35      0.113      0.103     0.0479      0.201     0.0918     0.0761     0.0283\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/50         0G      2.413      3.815      2.875      1.842         37        640: 100%|██████████| 63/63 [05:57<00:00,  5.67s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:03<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.183      0.139     0.0908     0.0421      0.163       0.12     0.0774     0.0304\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/50         0G      2.429      3.823      2.855      1.816         48        640: 100%|██████████| 63/63 [05:54<00:00,  5.63s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:03<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.272      0.148      0.126     0.0576      0.268      0.135      0.105     0.0411\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/50         0G      2.397      3.755      2.783      1.804         43        640: 100%|██████████| 63/63 [05:52<00:00,  5.59s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:00<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.375      0.175      0.153     0.0697      0.318       0.15      0.118     0.0465\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/50         0G      2.395      3.709      2.722      1.791         48        640: 100%|██████████| 63/63 [05:43<00:00,  5.46s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:00<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.277      0.176      0.139      0.064      0.239      0.158       0.11     0.0405\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/50         0G      2.344      3.706      2.726      1.777         45        640: 100%|██████████| 63/63 [05:45<00:00,  5.48s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:58<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.326      0.181      0.149     0.0722      0.304      0.161      0.122      0.049\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/50         0G      2.362       3.74      2.759      1.792         42        640: 100%|██████████| 63/63 [05:39<00:00,  5.39s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:03<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.292      0.181      0.162     0.0755      0.262      0.161      0.123     0.0461\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/50         0G      2.353      3.719      2.678      1.757         48        640: 100%|██████████| 63/63 [05:34<00:00,  5.31s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:58<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.307      0.156      0.131      0.064      0.283      0.131      0.104     0.0415\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/50         0G      2.339      3.695      2.673      1.773         30        640: 100%|██████████| 63/63 [05:41<00:00,  5.42s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:01<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.239      0.209      0.149     0.0698      0.263      0.147      0.116     0.0444\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/50         0G      2.289      3.659      2.647      1.734         58        640: 100%|██████████| 63/63 [05:40<00:00,  5.41s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:59<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.331      0.202      0.179     0.0857      0.336      0.178      0.145     0.0587\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/50         0G      2.283      3.642      2.631      1.736         45        640: 100%|██████████| 63/63 [05:37<00:00,  5.35s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:57<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.343      0.188      0.178     0.0876      0.432      0.165      0.148     0.0612\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/50         0G      2.296      3.648      2.586      1.723         55        640: 100%|██████████| 63/63 [05:32<00:00,  5.28s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:58<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.334      0.202      0.182     0.0902      0.405      0.169      0.162     0.0633\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/50         0G      2.263      3.637      2.558      1.715         43        640: 100%|██████████| 63/63 [05:32<00:00,  5.28s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:00<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.356      0.204      0.199      0.104      0.348      0.181      0.166     0.0694\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/50         0G      2.213      3.562      2.517      1.685         39        640: 100%|██████████| 63/63 [05:45<00:00,  5.48s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:58<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.411      0.222      0.217      0.107      0.382      0.185      0.167     0.0685\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/50         0G      2.234      3.571      2.522      1.692         63        640: 100%|██████████| 63/63 [05:28<00:00,  5.21s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:57<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.478      0.219      0.211     0.0997      0.438      0.188      0.165     0.0643\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/50         0G      2.217      3.595      2.524      1.675         40        640: 100%|██████████| 63/63 [05:35<00:00,  5.33s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:01<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.313      0.243      0.219      0.101      0.312      0.192      0.166      0.065\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/50         0G      2.226       3.57      2.521      1.687         57        640: 100%|██████████| 63/63 [05:33<00:00,  5.29s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:59<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.425      0.251       0.23      0.117      0.381      0.208      0.179     0.0732\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/50         0G      2.179      3.532      2.458      1.653         57        640: 100%|██████████| 63/63 [05:39<00:00,  5.39s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:58<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931       0.42       0.27      0.256      0.131      0.406      0.213      0.192     0.0813\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/50         0G      2.171      3.515      2.457      1.665         38        640: 100%|██████████| 63/63 [05:30<00:00,  5.25s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:58<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.399      0.259      0.237      0.125      0.396      0.221      0.191     0.0807\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/50         0G       2.16      3.539      2.478      1.648         31        640: 100%|██████████| 63/63 [05:43<00:00,  5.45s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:57<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.452      0.271      0.281      0.149      0.451      0.228       0.21     0.0911\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      31/50         0G      2.142      3.505      2.388       1.65         54        640: 100%|██████████| 63/63 [05:29<00:00,  5.22s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:00<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.509      0.281      0.276      0.146      0.487      0.237      0.222     0.0898\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      32/50         0G      2.163      3.472      2.402      1.649         65        640: 100%|██████████| 63/63 [05:42<00:00,  5.43s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:58<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.423      0.267       0.26      0.135      0.369      0.224      0.194     0.0802\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      33/50         0G      2.095      3.442      2.346      1.627         57        640: 100%|██████████| 63/63 [05:32<00:00,  5.28s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:57<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.471      0.292      0.288      0.154      0.457      0.238      0.221     0.0923\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      34/50         0G      2.121       3.47      2.361       1.63         55        640: 100%|██████████| 63/63 [05:32<00:00,  5.28s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:56<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931       0.49      0.306      0.309      0.168      0.463      0.233      0.229      0.101\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      35/50         0G      2.103      3.439      2.338      1.629         38        640: 100%|██████████| 63/63 [05:33<00:00,  5.30s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:58<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.465      0.316      0.301      0.159      0.395      0.244      0.213     0.0898\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      36/50         0G      2.105      3.455      2.321      1.618         49        640: 100%|██████████| 63/63 [05:37<00:00,  5.35s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:58<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931       0.53      0.318      0.331      0.178      0.527      0.245      0.249       0.11\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      37/50         0G      2.076      3.417      2.289      1.586         54        640: 100%|██████████| 63/63 [05:42<00:00,  5.44s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:00<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.479      0.297      0.316      0.178      0.471      0.246      0.235      0.102\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      38/50         0G      2.059      3.403      2.264      1.608         38        640: 100%|██████████| 63/63 [05:31<00:00,  5.27s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:58<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.477      0.344      0.339      0.189      0.507      0.283      0.275      0.114\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      39/50         0G       2.04      3.384      2.261      1.578         60        640: 100%|██████████| 63/63 [05:42<00:00,  5.44s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:56<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.504      0.338      0.342      0.194      0.469      0.278      0.257      0.107\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      40/50         0G      2.055      3.356      2.255      1.592         44        640: 100%|██████████| 63/63 [05:34<00:00,  5.30s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:56<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.535      0.325      0.347      0.202      0.518      0.249       0.25       0.11\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "      41/50         0G      2.093       3.35      2.424      1.671         28        640: 100%|██████████| 63/63 [05:17<00:00,  5.05s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:00<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.538      0.292      0.319      0.176      0.458      0.236      0.229     0.0954\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      42/50         0G      2.018      3.233      2.259      1.634         24        640: 100%|██████████| 63/63 [05:28<00:00,  5.21s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:56<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931       0.55      0.348      0.361      0.202      0.583      0.241      0.261      0.111\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      43/50         0G      2.009      3.268      2.265      1.617         18        640: 100%|██████████| 63/63 [05:17<00:00,  5.05s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:56<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.561       0.33      0.358        0.2      0.504      0.272      0.268      0.111\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      44/50         0G      1.992      3.226       2.23      1.606         24        640: 100%|██████████| 63/63 [05:18<00:00,  5.06s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:57<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.503      0.366      0.371      0.214      0.559      0.273      0.282      0.121\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      45/50         0G      1.968      3.188      2.175      1.586         23        640: 100%|██████████| 63/63 [05:25<00:00,  5.17s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:59<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.584      0.357      0.384       0.22      0.522      0.282      0.282      0.119\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      46/50         0G      1.944      3.167      2.143      1.583         23        640: 100%|██████████| 63/63 [05:29<00:00,  5.24s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:00<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.568      0.355      0.385      0.226      0.543      0.273      0.284      0.124\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      47/50         0G      1.948      3.203      2.143      1.584         27        640: 100%|██████████| 63/63 [05:24<00:00,  5.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:58<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.599       0.35      0.385      0.234      0.569      0.297      0.306      0.129\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      48/50         0G      1.964      3.209      2.148      1.587         22        640: 100%|██████████| 63/63 [05:33<00:00,  5.29s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:58<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.633      0.352      0.394      0.236      0.601      0.293        0.3      0.129\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      49/50         0G      1.925      3.168      2.095       1.58         29        640: 100%|██████████| 63/63 [05:34<00:00,  5.31s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:58<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.609      0.369      0.407      0.245      0.556      0.291      0.304      0.129\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      50/50         0G      1.917      3.156      2.087      1.574         19        640: 100%|██████████| 63/63 [05:31<00:00,  5.26s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [01:00<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.637      0.348      0.401      0.239      0.571      0.293      0.305      0.129\n",
      "\n",
      "50 epochs completed in 5.626 hours.\n",
      "Optimizer stripped from C:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS\\yolov8n_train2\\weights\\last.pt, 6.7MB\n",
      "Optimizer stripped from C:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS\\yolov8n_train2\\weights\\best.pt, 6.7MB\n",
      "\n",
      "Validating C:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS\\yolov8n_train2\\weights\\best.pt...\n",
      "Ultralytics 8.3.152  Python-3.11.11 torch-2.7.1+cpu CPU (11th Gen Intel Core(TM) i7-11800H 2.30GHz)\n",
      "YOLOv8n-seg summary (fused): 85 layers, 3,259,039 parameters, 0 gradients, 12.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   3%|▎         | 1/32 [00:02<01:02,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   6%|▋         | 2/32 [00:04<01:01,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 32/32 [00:52<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1000       2931      0.608      0.369      0.407      0.244      0.558      0.286      0.304      0.129\n",
      "                    ac         48         50      0.616      0.579      0.605      0.405      0.464       0.38      0.405      0.197\n",
      "                    lc        103        109      0.495      0.339      0.358      0.218      0.367       0.22      0.197     0.0605\n",
      "                    ph        730       1202      0.717      0.453      0.502      0.303      0.738      0.424       0.48      0.222\n",
      "                    pc        406        524      0.609      0.139      0.177      0.099      0.639      0.134      0.168     0.0752\n",
      "                    tc        654       1046      0.603      0.337      0.391      0.196      0.581      0.273      0.269       0.09\n",
      "Speed: 0.7ms preprocess, 30.8ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\Users\\jenni\\Desktop\\YOLO_TRAIN_RESULTS\\yolov8n_train2\u001b[0m\n",
      "--- Training for method 'train' completed. Results saved to C:/Users/jenni/Desktop/YOLO_TRAIN_RESULTS/yolov8n_train ---\n",
      "Could not retrieve validation metrics from training results.\n",
      "\n",
      "All training processes finished.\n"
     ]
    }
   ],
   "source": [
    "#gpu 사용을 하도록\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import glob\n",
    "from ultralytics import YOLO\n",
    "import torch # Import torch to check for GPU availability\n",
    "\n",
    "\n",
    "BASE_AUGMENTED_PATH = \"C:/Users/jenni/Desktop/augmented_data1\" # 증강된 데이터의 최상위 경로 (poisson_harmonized 폴더가 이 아래에 있음)\n",
    "PROJECT_OUTPUT_PATH = \"C:/Users/jenni/Desktop/YOLO_TRAIN_RESULTS\" # YOLO 학습 결과가 저장될 경로\n",
    "\n",
    "\n",
    "def convert_labelme_to_yolo_segmentation(json_dir, output_txt_dir, img_w, img_h, label_to_id_map):\n",
    "    \"\"\"\n",
    "    Labelme JSON 파일들을 YOLO segmentation .txt 파일로 변환합니다.\n",
    "    Args:\n",
    "        json_dir (str): Labelme JSON 파일들이 있는 디렉토리 경로.\n",
    "        output_txt_dir (str): 변환된 YOLO .txt 파일들을 저장할 디렉토리 경로.\n",
    "        img_w (int): 원본 이미지의 너비.\n",
    "        img_h (int): 원본 이미지의 높이.\n",
    "        label_to_id_map (dict): 클래스 이름(label)을 ID(숫자)로 매핑하는 딕셔너리.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_txt_dir, exist_ok=True)\n",
    "    json_files = glob.glob(os.path.join(json_dir, \"*.json\"))\n",
    "\n",
    "    for json_path in json_files:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        txt_filename = os.path.basename(json_path).replace(\".json\", \".txt\")\n",
    "        txt_path = os.path.join(output_txt_dir, txt_filename)\n",
    "\n",
    "        yolo_lines = []\n",
    "        for shape in data['shapes']:\n",
    "            label_name = shape['label']\n",
    "            if label_name in label_to_id_map:\n",
    "                class_id = label_to_id_map[label_name]\n",
    "                points = shape['points']\n",
    "                \n",
    "                normalized_points = []\n",
    "                for x, y in points:\n",
    "                    # 좌표를 이미지 크기에 맞춰 정규화\n",
    "                    normalized_points.append(f\"{(x / img_w):.6f}\")\n",
    "                    normalized_points.append(f\"{(y / img_h):.6f}\")\n",
    "                \n",
    "                yolo_lines.append(f\"{class_id} {' '.join(normalized_points)}\")\n",
    "            else:\n",
    "                print(f\"Warning: Class '{label_name}' not found in label_to_id_map. Skipping shape in {json_path}\")\n",
    "\n",
    "        if yolo_lines:\n",
    "            with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "                f.write('\\n'.join(yolo_lines))\n",
    "        # else:\n",
    "        #    # 객체가 없는 이미지도 빈 .txt 파일로 저장하는 것이 YOLO 학습에 더 안전합니다.\n",
    "        #    with open(txt_path, 'w') as f:\n",
    "        #        pass # 빈 파일 생성\n",
    "\n",
    "\n",
    "def create_data_yaml(dataset_base_path, yaml_output_path, class_names):\n",
    "    \"\"\"\n",
    "    YOLOv8 학습을 위한 data.yaml 파일을 생성합니다.\n",
    "    Args:\n",
    "        dataset_base_path (str): 증강된 이미지와 라벨이 있는 상위 디렉토리 (예: 'C:/Users/USER/Desktop/augmented_data1/poisson_harmonized')\n",
    "        yaml_output_path (str): 생성될 data.yaml 파일의 전체 경로.\n",
    "        class_names (list): 데이터셋의 모든 클래스 이름 목록.\n",
    "    \"\"\"\n",
    "    images_path = os.path.join(dataset_base_path, \"images\")\n",
    "    labels_path = os.path.join(dataset_base_path, \"labels_yolo\") # YOLO 형식의 라벨 폴더\n",
    "\n",
    "    # 경로를 상대 경로가 아닌 절대 경로로 지정하여 혼란 방지\n",
    "    train_img_path = images_path\n",
    "    val_img_path = images_path # 검증 데이터가 따로 없으면 train과 동일하게 설정\n",
    "\n",
    "    nc = len(class_names)\n",
    "    names = class_names\n",
    "\n",
    "    yaml_content = f\"\"\"\n",
    "path: {dataset_base_path}\n",
    "train: {os.path.relpath(train_img_path, dataset_base_path)} # path 기준으로 상대 경로\n",
    "val: {os.path.relpath(val_img_path, dataset_base_path)}\n",
    "\n",
    "nc: {nc}\n",
    "names: {names}\n",
    "\"\"\"\n",
    "    # 실제 YOLO 라벨 경로를 data.yaml이 참조하도록 labels 폴더의 이름을 변경하거나 (labels -> labels_yolo),\n",
    "    # data.yaml 내부에서 라벨 경로를 명시적으로 지정해야 합니다.\n",
    "    # 일반적으로 YOLO는 'images' 폴더와 같은 레벨의 'labels' 폴더를 기대합니다.\n",
    "    # 그래서 'labels_yolo'라는 폴더명을 가정했습니다.\n",
    "\n",
    "    with open(yaml_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(yaml_content.strip())\n",
    "    print(f\"'{yaml_output_path}' file created successfully.\")\n",
    "\n",
    "\n",
    "def train_yolo_for_method(method, epochs=50):\n",
    "    \"\"\"\n",
    "    특정 증강 방식에 대해 YOLOv8 모델을 학습시키고, 검증 결과를 도출합니다.\n",
    "    Args:\n",
    "        method (str): 증강 방식 이름 (예: 'poisson_harmonized').\n",
    "        epochs (int): 학습할 에포크 수.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting YOLOv8 training and validation for method: {method} ---\")\n",
    "\n",
    "    # Check for GPU availability\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    if device == 'cpu':\n",
    "        print(\"Warning: No GPU found. Training will be significantly slower on CPU. \"\n",
    "              \"Ensure PyTorch with CUDA is installed correctly if you intend to use a GPU.\")\n",
    "\n",
    "\n",
    "    # 1. 각 증강 방식에 맞는 데이터셋 경로 설정\n",
    "    dataset_path = os.path.join(BASE_AUGMENTED_PATH, method)\n",
    "    images_path = os.path.join(dataset_path, \"images\")\n",
    "    labels_json_path = os.path.join(dataset_path, \"labels\") # Labelme JSON 라벨이 있는 곳\n",
    "    labels_yolo_path = os.path.join(dataset_path, \"labels_yolo\") # YOLO .txt 라벨을 저장할 곳\n",
    "\n",
    "    # 2. 클래스 정보를 미리 정의\n",
    "    label2id = {}\n",
    "    id2label = {}\n",
    "    try:\n",
    "        with open(\"classes.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            class_names_from_file = [line.strip() for line in f if line.strip()]\n",
    "        for i, name in enumerate(class_names_from_file):\n",
    "            label2id[name] = i\n",
    "            id2label[i] = name\n",
    "        print(f\"Loaded classes: {id2label}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'classes.txt' not found. Please ensure it's in the same directory or provide class info manually.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading classes.txt: {e}\")\n",
    "        return\n",
    "\n",
    "    # 이미지 크기를 추정하기 위해 첫 번째 이미지 파일을 읽습니다.\n",
    "    first_image_path = None\n",
    "    image_files = glob.glob(os.path.join(images_path, \"*.jpg\")) + glob.glob(os.path.join(images_path, \"*.png\"))\n",
    "    if image_files:\n",
    "        first_image_path = image_files[0]\n",
    "        img_temp = cv2.imread(first_image_path)\n",
    "        if img_temp is None:\n",
    "            print(f\"Error: Could not read image {first_image_path}. Check image file integrity.\")\n",
    "            return\n",
    "        img_h, img_w = img_temp.shape[:2]\n",
    "        print(f\"Detected image dimensions: W={img_w}, H={img_h}\")\n",
    "    else:\n",
    "        print(f\"Error: No images found in {images_path}. Cannot determine image dimensions for YOLO conversion.\")\n",
    "        return\n",
    "\n",
    "    # 3. Labelme JSON 라벨을 YOLO Segmentation .txt 형식으로 변환\n",
    "    print(f\"Converting Labelme JSON to YOLO segmentation .txt for {method}...\")\n",
    "    convert_labelme_to_yolo_segmentation(labels_json_path, labels_yolo_path, img_w, img_h, label2id)\n",
    "    print(\"Conversion complete.\")\n",
    "\n",
    "    # 4. data.yaml 파일 생성\n",
    "    yaml_path = os.path.join(dataset_path, \"data.yaml\")\n",
    "    create_data_yaml(dataset_path, yaml_path, list(id2label.values()))\n",
    "\n",
    "    # 5. 항상 동일한 사전 학습 모델로부터 학습 시작\n",
    "    model = YOLO(\"yolov8n-seg.pt\")\n",
    "    print(f\"Loaded YOLOv8n-seg model.\")\n",
    "\n",
    "    # 6. 모든 학습에 동일한 하이퍼파라미터 적용\n",
    "    try:\n",
    "        # 학습 수행\n",
    "        results = model.train(\n",
    "            data=yaml_path,\n",
    "            epochs=epochs,\n",
    "            imgsz=640,\n",
    "            name=f\"yolov8n_{method}\",\n",
    "            project=PROJECT_OUTPUT_PATH, # 결과 저장 최상위 경로\n",
    "            save=True,\n",
    "            resume=False,\n",
    "            device=device # Explicitly set device\n",
    "        )\n",
    "        print(f\"--- Training for method '{method}' completed. Results saved to {PROJECT_OUTPUT_PATH}/yolov8n_{method} ---\")\n",
    "\n",
    "        # 7. 학습 완료 후 검증 수행 (자동으로 수행되지만 명시적으로 호출 가능)\n",
    "        if hasattr(results, 'metrics') and results.metrics:\n",
    "            print(\"\\n--- Validation Metrics ---\")\n",
    "            \n",
    "            precision = results.metrics.get('metrics/precision(B)', 'N/A')\n",
    "            recall = results.metrics.get('metrics/recall(B)', 'N/A')\n",
    "            mAP50 = results.metrics.get('metrics/mAP50(B)', 'N/A')\n",
    "            mAP = results.metrics.get('metrics/mAP50-95(B)', 'N/A') # mAP (0.5 to 0.95 IOU)\n",
    "\n",
    "            # F1-score 계산\n",
    "            f1_score = 'N/A'\n",
    "            if precision != 'N/A' and recall != 'N/A' and (precision + recall) > 0:\n",
    "                f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "            print(f\"Precision: {precision:.4f}\")\n",
    "            print(f\"Recall: {recall:.4f}\")\n",
    "            print(f\"F1-Score: {f1_score:.4f}\")\n",
    "            print(f\"mAP50: {mAP50:.4f}\")\n",
    "            print(f\"mAP (50-95): {mAP:.4f}\")\n",
    "            print(\"------------------------\")\n",
    "        else:\n",
    "            print(\"Could not retrieve validation metrics from training results.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during YOLO training for method '{method}': {e}\")\n",
    "\n",
    "\n",
    "# --- 사용 예시 ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 학습을 시작할 증강 방식 이름 지정 (예: 이전 증강 스크립트에서 사용한 blending mode와 동일하게)\n",
    "    target_method = \"train\" # ⚠️ 여기에 실제로 사용한 증강 방식 폴더 이름을 입력하세요.\n",
    "\n",
    "    train_yolo_for_method(target_method, epochs=50)\n",
    "\n",
    "    print(\"\\nAll training processes finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "144580ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting YOLOv8 training for method: train ---\n",
      "Using device: cpu\n",
      "Warning: No GPU found. Training will be significantly slower on CPU. Ensure PyTorch with CUDA is installed correctly if you intend to use a GPU.\n",
      "Loaded classes: {0: 'ac', 1: 'lc', 2: 'ph', 3: 'pc', 4: 'tc'}\n",
      "Converting Labelme JSON to YOLO segmentation .txt for training data (train)...\n",
      "Warning: No JSON files found in C:/Users/jenni/Desktop/augmented_data1\\train\\labels. No YOLO labels will be generated.\n",
      "Training data conversion complete.\n",
      "Converting Labelme JSON to YOLO segmentation .txt for validation data...\n",
      "Validation data conversion complete.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/jenni/Desktop/YOLO_TRAIN_RESULTS_with_val\\\\train_data.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 321\u001b[39m\n\u001b[32m    318\u001b[39m train_method_name = \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m \n\u001b[32m    320\u001b[39m \u001b[38;5;66;03m# 1. Train the model using the augmented data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m trained_model_path = \u001b[43mtrain_yolo_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_method_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[38;5;66;03m# 2. After training, validate the model using the separate validation set\u001b[39;00m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trained_model_path:\n\u001b[32m    325\u001b[39m     \u001b[38;5;66;03m# Create a data.yaml specifically for validation, pointing to the validation data\u001b[39;00m\n\u001b[32m    326\u001b[39m     \u001b[38;5;66;03m# We need to recreate it because the 'train' method's data.yaml might be in the run directory\u001b[39;00m\n\u001b[32m    327\u001b[39m     \u001b[38;5;66;03m# and we want to ensure the validation step explicitly uses the correct validation paths.\u001b[39;00m\n\u001b[32m    328\u001b[39m \n\u001b[32m    329\u001b[39m     \u001b[38;5;66;03m# Reload class info to ensure consistency\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 203\u001b[39m, in \u001b[36mtrain_yolo_model\u001b[39m\u001b[34m(train_method, epochs)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;66;03m# 6. data.yaml 파일 생성 (학습과 검증 경로를 모두 포함)\u001b[39;00m\n\u001b[32m    202\u001b[39m data_yaml_path = os.path.join(PROJECT_OUTPUT_PATH, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_data.yaml\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m \u001b[43mcreate_data_yaml\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43myaml_output_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_yaml_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_img_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_images_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_img_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_images_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_label_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_labels_yolo_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_label_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_labels_yolo_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_names\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# 7. 항상 동일한 사전 학습 모델로부터 학습 시작\u001b[39;00m\n\u001b[32m    213\u001b[39m model = YOLO(\u001b[33m\"\u001b[39m\u001b[33myolov8n-seg.pt\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mcreate_data_yaml\u001b[39m\u001b[34m(yaml_output_path, train_img_dir, val_img_dir, train_label_dir, val_label_dir, class_names)\u001b[39m\n\u001b[32m     90\u001b[39m     \u001b[38;5;66;03m# YOLOv8는 train/val/test 이미지가 하나의 'path' 아래에 상대 경로로 있거나,\u001b[39;00m\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# 각 train/val/test 경로를 절대 경로로 직접 지정할 수 있습니다.\u001b[39;00m\n\u001b[32m     92\u001b[39m     \u001b[38;5;66;03m# 여기서는 절대 경로를 사용하여 명확하게 지정합니다.\u001b[39;00m\n\u001b[32m     93\u001b[39m     yaml_content = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[33m# Dataset root directory (not strictly needed if train/val/test paths are absolute)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[33m# path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.path.dirname(train_img_dir)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    105\u001b[39m \u001b[33mnames: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnames\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43myaml_output_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    108\u001b[39m         f.write(yaml_content.strip())\n\u001b[32m    109\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myaml_output_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m file created successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jenni\\anaconda3\\envs\\aug\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:327\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    322\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    325\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:/Users/jenni/Desktop/YOLO_TRAIN_RESULTS_with_val\\\\train_data.yaml'"
     ]
    }
   ],
   "source": [
    "#val 데이터를 활용하여 지표 검증증\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import glob\n",
    "from ultralytics import YOLO\n",
    "import torch # Import torch to check for GPU availability\n",
    "\n",
    "# --- Paths ---\n",
    "BASE_AUGMENTED_PATH = \"C:/Users/jenni/Desktop/augmented_data1\" # 증강된 데이터의 최상위 경로 (train 폴더가 이 아래에 있음)\n",
    "# Assuming your original dataset has a 'val' split that you want to use for validation\n",
    "VAL_DATA_BASE_PATH = \"C:/Users/jenni/Desktop/original_data1\" # ⚠️ 여기에 실제 원본 데이터셋의 최상위 경로를 입력하세요.\n",
    "                                                              # 이 경로 아래에 'val_images'와 'val_labels' 폴더가 있다고 가정합니다.\n",
    "PROJECT_OUTPUT_PATH = \"C:/Users/jenni/Desktop/YOLO_TRAIN_RESULTS_with_val\" # YOLO 학습 결과가 저장될 경로\n",
    "\n",
    "def convert_labelme_to_yolo_segmentation(json_dir, output_txt_dir, img_w, img_h, label_to_id_map):\n",
    "    \"\"\"\n",
    "    Labelme JSON 파일들을 YOLO segmentation .txt 파일로 변환합니다.\n",
    "    Args:\n",
    "        json_dir (str): Labelme JSON 파일들이 있는 디렉토리 경로.\n",
    "        output_txt_dir (str): 변환된 YOLO .txt 파일들을 저장할 디렉토리 경로.\n",
    "        img_w (int): 원본 이미지의 너비.\n",
    "        img_h (int): 원본 이미지의 높이.\n",
    "        label_to_id_map (dict): 클래스 이름(label)을 ID(숫자)로 매핑하는 딕셔너리.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_txt_dir, exist_ok=True)\n",
    "    json_files = glob.glob(os.path.join(json_dir, \"*.json\"))\n",
    "\n",
    "    if not json_files:\n",
    "        print(f\"Warning: No JSON files found in {json_dir}. No YOLO labels will be generated.\")\n",
    "        return\n",
    "\n",
    "    for json_path in json_files:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        txt_filename = os.path.basename(json_path).replace(\".json\", \".txt\")\n",
    "        txt_path = os.path.join(output_txt_dir, txt_filename)\n",
    "\n",
    "        yolo_lines = []\n",
    "        # Ensure image dimensions are valid before normalization\n",
    "        if img_w <= 0 or img_h <= 0:\n",
    "            print(f\"Error: Invalid image dimensions (W={img_w}, H={img_h}) detected for {json_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        for shape in data['shapes']:\n",
    "            label_name = shape['label']\n",
    "            if label_name in label_to_id_map:\n",
    "                class_id = label_to_id_map[label_name]\n",
    "                points = shape['points']\n",
    "                \n",
    "                normalized_points = []\n",
    "                for x, y in points:\n",
    "                    # Clip coordinates to be within image bounds before normalization\n",
    "                    x = max(0.0, min(float(img_w), float(x)))\n",
    "                    y = max(0.0, min(float(img_h), float(y)))\n",
    "                    \n",
    "                    # Normalize coordinates\n",
    "                    normalized_points.append(f\"{(x / img_w):.6f}\")\n",
    "                    normalized_points.append(f\"{(y / img_h):.6f}\")\n",
    "                \n",
    "                yolo_lines.append(f\"{class_id} {' '.join(normalized_points)}\")\n",
    "            else:\n",
    "                print(f\"Warning: Class '{label_name}' not found in label_to_id_map. Skipping shape in {json_path}\")\n",
    "\n",
    "        if yolo_lines:\n",
    "            with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "                f.write('\\n'.join(yolo_lines))\n",
    "        else:\n",
    "            # If no objects are found or processed, create an empty .txt file\n",
    "            # This is important for YOLO to understand that the image has no objects.\n",
    "            with open(txt_path, 'w') as f:\n",
    "                pass\n",
    "\n",
    "\n",
    "def create_data_yaml(yaml_output_path, train_img_dir, val_img_dir, train_label_dir, val_label_dir, class_names):\n",
    "    \"\"\"\n",
    "    YOLOv8 학습 및 검증을 위한 data.yaml 파일을 생성합니다.\n",
    "    Args:\n",
    "        yaml_output_path (str): 생성될 data.yaml 파일의 전체 경로.\n",
    "        train_img_dir (str): 학습 이미지 디렉토리의 절대 경로.\n",
    "        val_img_dir (str): 검증 이미지 디렉토리의 절대 경로.\n",
    "        train_label_dir (str): 학습 YOLO .txt 라벨 디렉토리의 절대 경로.\n",
    "        val_label_dir (str): 검증 YOLO .txt 라벨 디렉토리의 절대 경로.\n",
    "        class_names (list): 데이터셋의 모든 클래스 이름 목록.\n",
    "    \"\"\"\n",
    "    nc = len(class_names)\n",
    "    names = class_names\n",
    "\n",
    "    # YOLOv8는 train/val/test 이미지가 하나의 'path' 아래에 상대 경로로 있거나,\n",
    "    # 각 train/val/test 경로를 절대 경로로 직접 지정할 수 있습니다.\n",
    "    # 여기서는 절대 경로를 사용하여 명확하게 지정합니다.\n",
    "    yaml_content = f\"\"\"\n",
    "# Dataset root directory (not strictly needed if train/val/test paths are absolute)\n",
    "# path: {os.path.dirname(train_img_dir)} \n",
    "\n",
    "train:\n",
    "  images: {train_img_dir}\n",
    "  labels: {train_label_dir}\n",
    "val:\n",
    "  images: {val_img_dir}\n",
    "  labels: {val_label_dir}\n",
    "\n",
    "nc: {nc}\n",
    "names: {names}\n",
    "\"\"\"\n",
    "    with open(yaml_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(yaml_content.strip())\n",
    "    print(f\"'{yaml_output_path}' file created successfully.\")\n",
    "\n",
    "\n",
    "def get_image_dimensions(image_dir):\n",
    "    \"\"\"\n",
    "    주어진 디렉토리의 첫 번째 이미지 파일에서 이미지 크기를 가져옵니다.\n",
    "    \"\"\"\n",
    "    image_files = glob.glob(os.path.join(image_dir, \"*.jpg\")) + glob.glob(os.path.join(image_dir, \"*.png\"))\n",
    "    if image_files:\n",
    "        first_image_path = image_files[0]\n",
    "        img_temp = cv2.imread(first_image_path)\n",
    "        if img_temp is None:\n",
    "            print(f\"Error: Could not read image {first_image_path}. Check image file integrity.\")\n",
    "            return None, None\n",
    "        return img_temp.shape[1], img_temp.shape[0] # width, height\n",
    "    else:\n",
    "        print(f\"Error: No images found in {image_dir}.\")\n",
    "        return None, None\n",
    "\n",
    "def load_class_info(class_file=\"classes.txt\"):\n",
    "    \"\"\"\n",
    "    'classes.txt' 파일에서 클래스 이름과 ID 매핑을 로드합니다.\n",
    "    \"\"\"\n",
    "    label2id = {}\n",
    "    id2label = {}\n",
    "    try:\n",
    "        with open(class_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            class_names_from_file = [line.strip() for line in f if line.strip()]\n",
    "        for i, name in enumerate(class_names_from_file):\n",
    "            label2id[name] = i\n",
    "            id2label[i] = name\n",
    "        print(f\"Loaded classes: {id2label}\")\n",
    "        return label2id, id2label, list(id2label.values())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: '{class_file}' not found. Please ensure it's in the same directory or provide class info manually.\")\n",
    "        return {}, {}, []\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {class_file}: {e}\")\n",
    "        return {}, {}, []\n",
    "\n",
    "\n",
    "def train_yolo_model(train_method, epochs=50):\n",
    "    \"\"\"\n",
    "    특정 증강 방식에 대해 YOLOv8 모델을 학습시킵니다.\n",
    "    학습 후, 저장된 모델 경로를 반환합니다.\n",
    "    Args:\n",
    "        train_method (str): 증강 방식 이름 (예: 'train').\n",
    "        epochs (int): 학습할 에포크 수.\n",
    "    Returns:\n",
    "        str: 학습된 모델이 저장될 경로 (예: 'runs/segment/yolov8n_train/weights/best.pt')\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting YOLOv8 training for method: {train_method} ---\")\n",
    "\n",
    "    # Check for GPU availability\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    if device == 'cpu':\n",
    "        print(\"Warning: No GPU found. Training will be significantly slower on CPU. \"\n",
    "              \"Ensure PyTorch with CUDA is installed correctly if you intend to use a GPU.\")\n",
    "\n",
    "    # 1. 학습 데이터셋 경로 설정 (증강된 데이터)\n",
    "    train_dataset_base_path = os.path.join(BASE_AUGMENTED_PATH, train_method)\n",
    "    train_images_path = os.path.join(train_dataset_base_path, \"images\")\n",
    "    train_labels_json_path = os.path.join(train_dataset_base_path, \"labels\") # Labelme JSON 라벨이 있는 곳\n",
    "    train_labels_yolo_path = os.path.join(train_dataset_base_path, \"labels_yolo\") # YOLO .txt 라벨을 저장할 곳\n",
    "\n",
    "    # 2. 검증 데이터셋 경로 설정 (원본 데이터셋의 val 폴더)\n",
    "    val_images_path = os.path.join(VAL_DATA_BASE_PATH, \"val_images\")\n",
    "    val_labels_json_path = os.path.join(VAL_DATA_BASE_PATH, \"val_labels\")\n",
    "    val_labels_yolo_path = os.path.join(VAL_DATA_BASE_PATH, \"val_labels_yolo\")\n",
    "\n",
    "    # 3. 클래스 정보 로드\n",
    "    label2id, id2label, class_names = load_class_info()\n",
    "    if not class_names: return None # Exit if classes are not loaded\n",
    "\n",
    "    # 4. 학습 및 검증 이미지 크기 추정 (YOLO 변환에 필요)\n",
    "    train_img_w, train_img_h = get_image_dimensions(train_images_path)\n",
    "    val_img_w, val_img_h = get_image_dimensions(val_images_path)\n",
    "\n",
    "    if train_img_w is None or val_img_w is None:\n",
    "        print(\"Error: Could not determine image dimensions for training or validation. Exiting.\")\n",
    "        return None\n",
    "\n",
    "    # 5. Labelme JSON 라벨을 YOLO Segmentation .txt 형식으로 변환 (학습 및 검증 데이터셋 모두)\n",
    "    print(f\"Converting Labelme JSON to YOLO segmentation .txt for training data ({train_method})...\")\n",
    "    convert_labelme_to_yolo_segmentation(train_labels_json_path, train_labels_yolo_path, train_img_w, train_img_h, label2id)\n",
    "    print(\"Training data conversion complete.\")\n",
    "\n",
    "    print(f\"Converting Labelme JSON to YOLO segmentation .txt for validation data...\")\n",
    "    convert_labelme_to_yolo_segmentation(val_labels_json_path, val_labels_yolo_path, val_img_w, val_img_h, label2id)\n",
    "    print(\"Validation data conversion complete.\")\n",
    "\n",
    "    # 6. data.yaml 파일 생성 (학습과 검증 경로를 모두 포함)\n",
    "    data_yaml_path = os.path.join(PROJECT_OUTPUT_PATH, f\"{train_method}_data.yaml\")\n",
    "    create_data_yaml(\n",
    "        yaml_output_path=data_yaml_path,\n",
    "        train_img_dir=train_images_path,\n",
    "        val_img_dir=val_images_path,\n",
    "        train_label_dir=train_labels_yolo_path,\n",
    "        val_label_dir=val_labels_yolo_path,\n",
    "        class_names=class_names\n",
    "    )\n",
    "\n",
    "    # 7. 항상 동일한 사전 학습 모델로부터 학습 시작\n",
    "    model = YOLO(\"yolov8n-seg.pt\")\n",
    "    print(f\"Loaded YOLOv8n-seg model.\")\n",
    "\n",
    "    # 8. 학습 수행\n",
    "    try:\n",
    "        # YOLOv8 train function automatically performs validation if 'val' path is given in data.yaml\n",
    "        results = model.train(\n",
    "            data=data_yaml_path,\n",
    "            epochs=epochs,\n",
    "            imgsz=640,\n",
    "            name=f\"yolov8n_{train_method}\",\n",
    "            project=PROJECT_OUTPUT_PATH, # 결과 저장 최상위 경로\n",
    "            save=True,\n",
    "            resume=False,\n",
    "            device=device\n",
    "        )\n",
    "        print(f\"--- Training for method '{train_method}' completed. ---\")\n",
    "\n",
    "        # Get the path to the best trained model\n",
    "        # The 'path' attribute of the returned results object points to the run directory\n",
    "        # The 'best.pt' model is usually saved within this directory.\n",
    "        run_dir = os.path.join(PROJECT_OUTPUT_PATH, f\"yolov8n_{train_method}\")\n",
    "        best_model_path = os.path.join(run_dir, \"weights\", \"best.pt\")\n",
    "        if os.path.exists(best_model_path):\n",
    "            print(f\"Best model saved to: {best_model_path}\")\n",
    "            return best_model_path\n",
    "        else:\n",
    "            print(f\"Warning: 'best.pt' not found at {best_model_path}. Check training logs for exact path.\")\n",
    "            # Fallback to last.pt if best.pt is not found (though best.pt is preferred)\n",
    "            last_model_path = os.path.join(run_dir, \"weights\", \"last.pt\")\n",
    "            if os.path.exists(last_model_path):\n",
    "                print(f\"Using 'last.pt' model at: {last_model_path}\")\n",
    "                return last_model_path\n",
    "            else:\n",
    "                print(f\"Error: Neither 'best.pt' nor 'last.pt' found in {run_dir}.\")\n",
    "                return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during YOLO training for method '{train_method}': {e}\")\n",
    "        return None\n",
    "\n",
    "def validate_yolo_model(trained_model_path, validation_data_yaml_path):\n",
    "    \"\"\"\n",
    "    학습된 YOLOv8 모델을 사용하여 별도의 검증 데이터셋에 대해 성능을 평가합니다.\n",
    "    Args:\n",
    "        trained_model_path (str): 학습된 모델 파일의 경로 (예: 'runs/segment/yolov8n_train/weights/best.pt').\n",
    "        validation_data_yaml_path (str): 검증 데이터셋에 대한 data.yaml 파일 경로.\n",
    "    \"\"\"\n",
    "    if not trained_model_path or not os.path.exists(trained_model_path):\n",
    "        print(f\"Error: Trained model not found at {trained_model_path}. Cannot perform validation.\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(validation_data_yaml_path):\n",
    "        print(f\"Error: Validation data YAML not found at {validation_data_yaml_path}. Cannot perform validation.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- Starting YOLOv8 validation for model: {os.path.basename(trained_model_path)} ---\")\n",
    "    print(f\"Using validation data specified in: {validation_data_yaml_path}\")\n",
    "\n",
    "    # Load the trained model\n",
    "    model = YOLO(trained_model_path)\n",
    "    print(f\"Loaded trained model: {trained_model_path}\")\n",
    "\n",
    "    # Perform validation\n",
    "    try:\n",
    "        metrics = model.val(data=validation_data_yaml_path)\n",
    "\n",
    "        print(\"\\n--- Validation Metrics ---\")\n",
    "        precision = metrics.box.p # Precision for bounding box detection\n",
    "        recall = metrics.box.r    # Recall for bounding box detection\n",
    "        mAP50 = metrics.box.map50 # mAP@0.5 for bounding box detection\n",
    "        mAP = metrics.box.map     # mAP@0.5-0.95 for bounding box detection\n",
    "\n",
    "        # Segmentation metrics (often denoted with seg in Ultralytics)\n",
    "        # Note: If your model is a segmentation model (e.g., yolov8n-seg), these will be available.\n",
    "        # If it's a detection model, these might be 0 or not present.\n",
    "        seg_mAP50 = getattr(metrics.seg, 'map50', 'N/A')\n",
    "        seg_mAP = getattr(metrics.seg, 'map', 'N/A')\n",
    "\n",
    "        # F1-score calculation (for bounding box detection)\n",
    "        f1_score = 'N/A'\n",
    "        if precision is not None and recall is not None and (precision + recall) > 0:\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        print(f\"Box Precision: {precision:.4f}\")\n",
    "        print(f\"Box Recall: {recall:.4f}\")\n",
    "        print(f\"Box F1-Score: {f1_score:.4f}\")\n",
    "        print(f\"Box mAP50: {mAP50:.4f}\")\n",
    "        print(f\"Box mAP (50-95): {mAP:.4f}\")\n",
    "        \n",
    "        if seg_mAP50 != 'N/A': # Check if segmentation metrics are available\n",
    "            print(f\"Seg mAP50: {seg_mAP50:.4f}\")\n",
    "            print(f\"Seg mAP (50-95): {seg_mAP:.4f}\")\n",
    "        else:\n",
    "            print(\"Segmentation metrics not available (model might not be segmentation, or no masks found).\")\n",
    "            \n",
    "        print(\"------------------------\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during YOLO validation: {e}\")\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 학습을 시작할 증강 방식 이름 지정 (예: 이전 증강 스크립트에서 사용한 blending mode와 동일하게)\n",
    "    # BASE_AUGMENTED_PATH/train 폴더에 학습 데이터가 있다고 가정\n",
    "    train_method_name = \"train\" \n",
    "\n",
    "    # 1. Train the model using the augmented data\n",
    "    trained_model_path = train_yolo_model(train_method_name, epochs=50)\n",
    "\n",
    "    # 2. After training, validate the model using the separate validation set\n",
    "    if trained_model_path:\n",
    "        # Create a data.yaml specifically for validation, pointing to the validation data\n",
    "        # We need to recreate it because the 'train' method's data.yaml might be in the run directory\n",
    "        # and we want to ensure the validation step explicitly uses the correct validation paths.\n",
    "        \n",
    "        # Reload class info to ensure consistency\n",
    "        label2id, id2label, class_names = load_class_info()\n",
    "        if not class_names:\n",
    "            print(\"Failed to load class information. Cannot proceed with validation.\")\n",
    "        else:\n",
    "            val_images_path = os.path.join(VAL_DATA_BASE_PATH, \"val_images\")\n",
    "            val_labels_yolo_path = os.path.join(VAL_DATA_BASE_PATH, \"val_labels_yolo\")\n",
    "\n",
    "            # Validate images and labels exist\n",
    "            if not os.path.exists(val_images_path) or not os.path.exists(val_labels_yolo_path):\n",
    "                print(f\"Error: Validation images or labels path missing. Please check '{val_images_path}' and '{val_labels_yolo_path}'.\")\n",
    "            else:\n",
    "                # Get the image dimensions for the validation set, as YOLO conversion needs it.\n",
    "                # Assuming all images in val_images have the same size.\n",
    "                val_img_w, val_img_h = get_image_dimensions(val_images_path)\n",
    "                if val_img_w is None:\n",
    "                    print(\"Could not get dimensions for validation images. Skipping validation.\")\n",
    "                else:\n",
    "                    # Ensure validation labels are converted if they weren't already (e.g., if you run this script directly)\n",
    "                    # This step might be redundant if train_yolo_model already converted them, but it's safe.\n",
    "                    val_labels_json_path = os.path.join(VAL_DATA_BASE_PATH, \"val_labels\")\n",
    "                    convert_labelme_to_yolo_segmentation(val_labels_json_path, val_labels_yolo_path, val_img_w, val_img_h, label2id)\n",
    "                    \n",
    "                    # Create a data.yaml for validation specifically\n",
    "                    validation_data_yaml_path = os.path.join(PROJECT_OUTPUT_PATH, f\"{train_method}_val_data.yaml\")\n",
    "                    create_data_yaml(\n",
    "                        yaml_output_path=validation_data_yaml_path,\n",
    "                        train_img_dir=os.path.join(BASE_AUGMENTED_PATH, train_method, \"images\"), # Placeholder, not used by val\n",
    "                        val_img_dir=val_images_path,\n",
    "                        train_label_dir=os.path.join(BASE_AUGMENTED_PATH, train_method, \"labels_yolo\"), # Placeholder\n",
    "                        val_label_dir=val_labels_yolo_path,\n",
    "                        class_names=class_names\n",
    "                    )\n",
    "                    validate_yolo_model(trained_model_path, validation_data_yaml_path)\n",
    "    else:\n",
    "        print(\"Training did not complete successfully, skipping validation.\")\n",
    "\n",
    "    print(\"\\nAll processes finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92a9a8a-b119-4efd-80c0-052b9f340ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63332f21-5f13-466e-a323-d55470fdcb96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2350 image files.\n",
      "Selected 100 image files for sampling.\n",
      "Warning: Labelme JSON file not found for aug_d69b056a3468.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_d69b056a3468.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_d69b056a3468.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_d69b056a3468.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_18860d25f530.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_18860d25f530.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_18860d25f530.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_18860d25f530.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_80e716910904.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_80e716910904.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_80e716910904.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_80e716910904.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_61977048c50c.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_61977048c50c.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_61977048c50c.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_61977048c50c.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_44d0ed764b83.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_44d0ed764b83.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_44d0ed764b83.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_44d0ed764b83.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_de37f508f3ef.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_de37f508f3ef.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_de37f508f3ef.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_de37f508f3ef.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_e594aefa062e.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_e594aefa062e.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_e594aefa062e.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_e594aefa062e.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_4b5603a6294a.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_4b5603a6294a.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_4b5603a6294a.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_4b5603a6294a.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_3edf4d6d7abc.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_3edf4d6d7abc.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_3edf4d6d7abc.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_3edf4d6d7abc.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_cdd9fb8bc009.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_cdd9fb8bc009.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_cdd9fb8bc009.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_cdd9fb8bc009.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_f341d07d2da7.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_f341d07d2da7.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_f341d07d2da7.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_f341d07d2da7.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_9b32e3d099ca.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_9b32e3d099ca.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_9b32e3d099ca.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_9b32e3d099ca.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_03390295ccdd.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_03390295ccdd.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_03390295ccdd.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_03390295ccdd.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_9c75d7743592.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_9c75d7743592.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_9c75d7743592.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_9c75d7743592.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_b5c42e5e86a3.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_b5c42e5e86a3.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_b5c42e5e86a3.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_b5c42e5e86a3.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_f444c916150c.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_f444c916150c.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_f444c916150c.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_f444c916150c.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_182ba106e9bd.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_182ba106e9bd.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_182ba106e9bd.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_182ba106e9bd.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_fe46b96ef426.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_fe46b96ef426.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_fe46b96ef426.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_fe46b96ef426.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_d0fb1028837c.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_d0fb1028837c.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_d0fb1028837c.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_d0fb1028837c.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_3c59f19392ce.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_3c59f19392ce.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_3c59f19392ce.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_3c59f19392ce.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_464a6e876a5e.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_464a6e876a5e.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_464a6e876a5e.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_464a6e876a5e.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_c52b70792dbe.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_c52b70792dbe.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_c52b70792dbe.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_c52b70792dbe.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_a08297818cda.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_a08297818cda.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_a08297818cda.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_a08297818cda.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_4b6e5a93597b.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_4b6e5a93597b.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_4b6e5a93597b.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_4b6e5a93597b.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_2d72f9905748.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_2d72f9905748.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_2d72f9905748.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_2d72f9905748.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_1cd91ef33bed.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_1cd91ef33bed.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_1cd91ef33bed.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_1cd91ef33bed.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_7c07225506d0.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_7c07225506d0.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_7c07225506d0.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_7c07225506d0.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_e01a2075dfb1.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_e01a2075dfb1.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_e01a2075dfb1.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_e01a2075dfb1.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_bd6656ed990b.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_bd6656ed990b.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_bd6656ed990b.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_bd6656ed990b.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_263ed9875348.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_263ed9875348.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_263ed9875348.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_263ed9875348.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_c027bf39755b.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_c027bf39755b.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_c027bf39755b.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_c027bf39755b.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_b27c3f78ac84.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_b27c3f78ac84.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_b27c3f78ac84.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_b27c3f78ac84.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_f97731affbc8.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_f97731affbc8.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_f97731affbc8.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_f97731affbc8.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_351ad8e2ee73.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_351ad8e2ee73.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_351ad8e2ee73.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_351ad8e2ee73.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_ffb0fd8f3604.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_ffb0fd8f3604.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_ffb0fd8f3604.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_ffb0fd8f3604.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_d0febcfa91b7.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_d0febcfa91b7.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_d0febcfa91b7.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_d0febcfa91b7.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_aca82facdbad.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_aca82facdbad.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_aca82facdbad.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_aca82facdbad.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_a8b924db158a.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_a8b924db158a.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_a8b924db158a.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_a8b924db158a.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_dfd3e3fb72b4.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_dfd3e3fb72b4.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_dfd3e3fb72b4.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_dfd3e3fb72b4.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_647dc8acc4dc.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_647dc8acc4dc.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_647dc8acc4dc.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_647dc8acc4dc.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_23ea4a091afc.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_23ea4a091afc.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_23ea4a091afc.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_23ea4a091afc.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_ebeee06c789a.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_ebeee06c789a.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_ebeee06c789a.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_ebeee06c789a.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_193141988407.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_193141988407.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_193141988407.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_193141988407.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_dae66e826262.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_dae66e826262.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_dae66e826262.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_dae66e826262.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_e073bce328ef.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_e073bce328ef.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_e073bce328ef.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_e073bce328ef.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_390e50de907f.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_390e50de907f.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_390e50de907f.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_390e50de907f.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_e47040a4431c.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_e47040a4431c.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_e47040a4431c.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_e47040a4431c.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_2fe0107efb49.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_2fe0107efb49.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_2fe0107efb49.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_2fe0107efb49.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_8049c55cf6a4.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_8049c55cf6a4.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_8049c55cf6a4.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_8049c55cf6a4.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_ebe9b9d167ea.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_ebe9b9d167ea.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_ebe9b9d167ea.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_ebe9b9d167ea.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_5564a6dcdff4.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_5564a6dcdff4.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_5564a6dcdff4.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_5564a6dcdff4.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_f5c8304021db.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_f5c8304021db.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_f5c8304021db.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_f5c8304021db.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_3a69d131d2dd.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_3a69d131d2dd.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_3a69d131d2dd.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_3a69d131d2dd.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_f8c3c966fbd0.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_f8c3c966fbd0.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_f8c3c966fbd0.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_f8c3c966fbd0.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_8e1270638eba.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_8e1270638eba.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_8e1270638eba.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_8e1270638eba.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_c6e7b88195e4.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_c6e7b88195e4.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_c6e7b88195e4.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_c6e7b88195e4.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_513b5828b411.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_513b5828b411.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_513b5828b411.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_513b5828b411.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_fee7a9b05fec.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_fee7a9b05fec.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_fee7a9b05fec.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_fee7a9b05fec.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_2f2d1ef89f4f.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_2f2d1ef89f4f.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_2f2d1ef89f4f.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_2f2d1ef89f4f.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_79f00033a86d.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_79f00033a86d.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_79f00033a86d.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_79f00033a86d.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_d9ba93d59522.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_d9ba93d59522.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_d9ba93d59522.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_d9ba93d59522.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_d96c403861fc.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_d96c403861fc.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_d96c403861fc.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_d96c403861fc.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_a9fe7ce1e8d2.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_a9fe7ce1e8d2.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_a9fe7ce1e8d2.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_a9fe7ce1e8d2.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_415be98541fd.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_415be98541fd.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_415be98541fd.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_415be98541fd.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_9778b459dd37.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_9778b459dd37.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_9778b459dd37.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_9778b459dd37.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_72d2e2423219.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_72d2e2423219.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_72d2e2423219.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_72d2e2423219.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_eec1b4fac5ad.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_eec1b4fac5ad.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_eec1b4fac5ad.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_eec1b4fac5ad.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_18ea882c9251.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_18ea882c9251.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_18ea882c9251.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_18ea882c9251.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_6ec4f38696c4.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_6ec4f38696c4.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_6ec4f38696c4.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_6ec4f38696c4.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_7f9a1c3a776c.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_7f9a1c3a776c.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_7f9a1c3a776c.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_7f9a1c3a776c.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_508c7eb8aea7.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_508c7eb8aea7.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_508c7eb8aea7.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_508c7eb8aea7.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_efb8d654a8b2.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_efb8d654a8b2.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_efb8d654a8b2.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_efb8d654a8b2.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_b3295de481ec.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_b3295de481ec.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_b3295de481ec.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_b3295de481ec.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_443ccb4786a0.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_443ccb4786a0.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_443ccb4786a0.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_443ccb4786a0.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_c3b4a72975d9.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_c3b4a72975d9.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_c3b4a72975d9.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_c3b4a72975d9.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_a865972262a4.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_a865972262a4.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_a865972262a4.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_a865972262a4.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_d8613f44fef7.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_d8613f44fef7.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_d8613f44fef7.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_d8613f44fef7.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_2c84acb5f70b.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_2c84acb5f70b.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_2c84acb5f70b.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_2c84acb5f70b.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_638eb84185c0.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_638eb84185c0.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_638eb84185c0.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_638eb84185c0.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_24e05c74fc5c.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_24e05c74fc5c.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_24e05c74fc5c.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_24e05c74fc5c.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_c7418cb00c00.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_c7418cb00c00.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_c7418cb00c00.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_c7418cb00c00.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_0037776a5f6c.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_0037776a5f6c.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_0037776a5f6c.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_0037776a5f6c.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_5f547815d21f.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_5f547815d21f.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_5f547815d21f.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_5f547815d21f.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_3582e6262c8b.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_3582e6262c8b.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_3582e6262c8b.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_3582e6262c8b.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_a91325df968d.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_a91325df968d.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_a91325df968d.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_a91325df968d.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_dec246791b04.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_dec246791b04.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_dec246791b04.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_dec246791b04.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_6deb28367fe1.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_6deb28367fe1.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_6deb28367fe1.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_6deb28367fe1.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_1e5973d9b179.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_1e5973d9b179.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_1e5973d9b179.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_1e5973d9b179.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_57edd60de9e8.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_57edd60de9e8.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_57edd60de9e8.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_57edd60de9e8.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_85cb809f551e.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_85cb809f551e.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_85cb809f551e.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_85cb809f551e.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_3334a3ee1d78.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_3334a3ee1d78.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_3334a3ee1d78.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_3334a3ee1d78.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_e3becd34630a.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_e3becd34630a.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_e3becd34630a.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_e3becd34630a.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_2258d88fde21.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_2258d88fde21.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_2258d88fde21.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_2258d88fde21.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_401e270557be.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_401e270557be.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_401e270557be.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_401e270557be.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_c441d769c9b5.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_c441d769c9b5.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_c441d769c9b5.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_c441d769c9b5.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_a7b7c7a1724c.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_a7b7c7a1724c.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_a7b7c7a1724c.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_a7b7c7a1724c.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_89b8d28a2d06.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_89b8d28a2d06.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_89b8d28a2d06.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_89b8d28a2d06.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_eb96c941b026.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_eb96c941b026.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_eb96c941b026.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_eb96c941b026.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_57f31623f499.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_57f31623f499.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_57f31623f499.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_57f31623f499.txt. Skipping.\n",
      "Warning: Labelme JSON file not found for aug_8f5a1efd391e.jpg at C:/Users/USER/Desktop/augmented1/train\\labels\\aug_8f5a1efd391e.json. Skipping.\n",
      "Warning: YOLO .txt file not found for aug_8f5a1efd391e.jpg at C:/Users/USER/Desktop/augmented1/train\\labels_yolo\\aug_8f5a1efd391e.txt. Skipping.\n",
      "\n",
      "--- Successfully copied 100 image-label pairs. ---\n",
      "Samples are saved in: C:/Users/USER/Desktop/train_100_samples\n"
     ]
    }
   ],
   "source": [
    "#json 파일일 떄 \n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "\n",
    "# 원본 train 폴더 경로 (images와 labels 폴더가 이 안에 있다고 가정)\n",
    "SOURCE_TRAIN_DIR = \"C:/Users/USER/Desktop/augmented1/train\"\n",
    "\n",
    "# 추출된 파일들을 저장할 새 폴더 경로\n",
    "DESTINATION_DIR = \"C:/Users/USER/Desktop/train_100_samples\"\n",
    "\n",
    "# 추출할 파일 개수\n",
    "NUM_SAMPLES = 100\n",
    "\n",
    "def extract_samples(source_base_dir, dest_base_dir, num_samples=100):\n",
    "    \"\"\"\n",
    "    원본 폴더에서 이미지와 해당 라벨 파일을 num_samples 개수만큼 추출하여 새 폴더로 복사합니다.\n",
    "    \"\"\"\n",
    "    source_images_dir = os.path.join(source_base_dir, \"images\")\n",
    "    source_labels_dir = os.path.join(source_base_dir, \"labels\") # Labelme JSON 라벨 폴더\n",
    "    source_labels_yolo_dir = os.path.join(source_base_dir, \"labels_yolo\") # YOLO .txt 라벨 폴더\n",
    "\n",
    "    dest_images_dir = os.path.join(dest_base_dir, \"images\")\n",
    "    dest_labels_dir = os.path.join(dest_base_dir, \"labels\")\n",
    "    dest_labels_yolo_dir = os.path.join(dest_base_dir, \"labels_yolo\")\n",
    "\n",
    "\n",
    "    # 대상 폴더 생성\n",
    "    os.makedirs(dest_images_dir, exist_ok=True)\n",
    "    os.makedirs(dest_labels_dir, exist_ok=True)\n",
    "    os.makedirs(dest_labels_yolo_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    # 이미지 파일 목록 가져오기 (jpg, png 모두 포함)\n",
    "    image_files = []\n",
    "    image_files.extend(glob.glob(os.path.join(source_images_dir, \"*.jpg\")))\n",
    "    image_files.extend(glob.glob(os.path.join(source_images_dir, \"*.png\")))\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"Error: No image files found in {source_images_dir}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(image_files)} image files.\")\n",
    "\n",
    "    # 충분한 파일이 있는지 확인\n",
    "    if len(image_files) < num_samples:\n",
    "        print(f\"Warning: Only {len(image_files)} image files found, less than requested {num_samples}. Copying all found files.\")\n",
    "        selected_images = image_files\n",
    "    else:\n",
    "        # 무작위로 num_samples 개수만큼 이미지 선택\n",
    "        selected_images = random.sample(image_files, num_samples)\n",
    "        # 또는 순서대로 선택하고 싶다면: selected_images = image_files[:num_samples]\n",
    "\n",
    "    print(f\"Selected {len(selected_images)} image files for sampling.\")\n",
    "\n",
    "    copied_count = 0\n",
    "    for img_path in selected_images:\n",
    "        img_filename = os.path.basename(img_path)\n",
    "        base_filename = os.path.splitext(img_filename)[0]\n",
    "\n",
    "        # 이미지 복사\n",
    "        dest_img_path = os.path.join(dest_images_dir, img_filename)\n",
    "        shutil.copy(img_path, dest_img_path)\n",
    "\n",
    "        # 해당 Labelme JSON 라벨 파일 복사\n",
    "        json_label_path = os.path.join(source_labels_dir, f\"{base_filename}.json\")\n",
    "        if os.path.exists(json_label_path):\n",
    "            dest_json_label_path = os.path.join(dest_labels_dir, f\"{base_filename}.json\")\n",
    "            shutil.copy(json_label_path, dest_json_label_path)\n",
    "        else:\n",
    "            print(f\"Warning: Labelme JSON file not found for {img_filename} at {json_label_path}. Skipping.\")\n",
    "\n",
    "        # 해당 YOLO .txt 라벨 파일 복사\n",
    "        yolo_label_path = os.path.join(source_labels_yolo_dir, f\"{base_filename}.txt\")\n",
    "        if os.path.exists(yolo_label_path):\n",
    "            dest_yolo_label_path = os.path.join(dest_labels_yolo_dir, f\"{base_filename}.txt\")\n",
    "            shutil.copy(yolo_label_path, dest_yolo_label_path)\n",
    "        else:\n",
    "            print(f\"Warning: YOLO .txt file not found for {img_filename} at {yolo_label_path}. Skipping.\")\n",
    "        \n",
    "        copied_count += 1\n",
    "\n",
    "    print(f\"\\n--- Successfully copied {copied_count} image-label pairs. ---\")\n",
    "    print(f\"Samples are saved in: {DESTINATION_DIR}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_samples(SOURCE_TRAIN_DIR, DESTINATION_DIR, NUM_SAMPLES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aug",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
